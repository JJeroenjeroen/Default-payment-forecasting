{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whatever will be used in this notebook\n",
    "import pylab\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import SMOTE to balance trainset \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import from sklearn\n",
    "#Estimators:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Set generation \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Performance metrics:\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#K-fold crossvalidation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings regarding the version of the pandas library which is used \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make variable for the imported models\n",
    "modelSVC = SVC()\n",
    "modelRF = RandomForestClassifier()\n",
    "modelLR = LogisticRegression()\n",
    "modelKNN = KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv file and remove the extra ID column\n",
    "credit = pd.read_csv(\"C:\\\\Users\\\\Jeroen\\\\Desktop\\\\Ubiqum\\\\Data Science\\\\Excel Files\\\\credit_3.csv\", header = 0)\n",
    "credit = credit[credit.columns[1:len(credit.columns)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAYSTAT_SEP</th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29461</th>\n",
       "      <td>0.017741</td>\n",
       "      <td>1.849291</td>\n",
       "      <td>0.164656</td>\n",
       "      <td>0.412625</td>\n",
       "      <td>1.132625</td>\n",
       "      <td>0.387036</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29462</th>\n",
       "      <td>-0.875875</td>\n",
       "      <td>-0.682263</td>\n",
       "      <td>-0.234720</td>\n",
       "      <td>-0.127385</td>\n",
       "      <td>-1.004498</td>\n",
       "      <td>0.822032</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29463</th>\n",
       "      <td>3.592206</td>\n",
       "      <td>-0.656821</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-1.053118</td>\n",
       "      <td>-0.758007</td>\n",
       "      <td>0.169538</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>19.664957</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29464</th>\n",
       "      <td>0.911357</td>\n",
       "      <td>-0.727253</td>\n",
       "      <td>4.803964</td>\n",
       "      <td>-0.667396</td>\n",
       "      <td>1.388081</td>\n",
       "      <td>0.604534</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>2.813944</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29465</th>\n",
       "      <td>0.017741</td>\n",
       "      <td>-0.057084</td>\n",
       "      <td>-0.220274</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>1.384032</td>\n",
       "      <td>1.148279</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PAYSTAT_SEP  CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  \\\n",
       "29461     0.017741     1.849291  0.164656  0.412625      1.132625  0.387036   \n",
       "29462    -0.875875    -0.682263 -0.234720 -0.127385     -1.004498  0.822032   \n",
       "29463     3.592206    -0.656821 -0.344828 -1.053118     -0.758007  0.169538   \n",
       "29464     0.911357    -0.727253  4.803964 -0.667396      1.388081  0.604534   \n",
       "29465     0.017741    -0.057084 -0.220274 -0.898829      1.384032  1.148279   \n",
       "\n",
       "       No_payment  Paid_in_time  Paid_partly  1_month_late  2_months_late  \\\n",
       "29461    -0.31168     -0.488987     0.999729     -0.355373      -0.315466   \n",
       "29462    -0.31168      2.045043    -1.000272     -0.355373      -0.315466   \n",
       "29463    -0.31168     -0.488987    -1.000272     -0.355373      -0.315466   \n",
       "29464    -0.31168     -0.488987    -1.000272      2.813944      -0.315466   \n",
       "29465    -0.31168     -0.488987     0.999729     -0.355373      -0.315466   \n",
       "\n",
       "       3_months_late  4_months_late  5_months_late  6_months_late  \\\n",
       "29461      -0.105112      -0.050852      -0.029718      -0.019325   \n",
       "29462      -0.105112      -0.050852      -0.029718      -0.019325   \n",
       "29463      -0.105112      19.664957      -0.029718      -0.019325   \n",
       "29464      -0.105112      -0.050852      -0.029718      -0.019325   \n",
       "29465      -0.105112      -0.050852      -0.029718      -0.019325   \n",
       "\n",
       "       7_months_late  8_months_late  DEFAULT  \n",
       "29461      -0.017479      -0.025401        1  \n",
       "29462      -0.017479      -0.025401        1  \n",
       "29463      -0.017479      -0.025401        0  \n",
       "29464      -0.017479      -0.025401        0  \n",
       "29465      -0.017479      -0.025401        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give head of the df so it is easily visible which vars should function as independent variable\n",
    "credit.head()\n",
    "credit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAYSTAT_SEP</th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.804974</td>\n",
       "      <td>-0.652117</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-1.130262</td>\n",
       "      <td>-0.651099</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>3.169916</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.875875</td>\n",
       "      <td>-0.668758</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-0.358818</td>\n",
       "      <td>-0.999086</td>\n",
       "      <td>-1.026701</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017741</td>\n",
       "      <td>-0.309746</td>\n",
       "      <td>-0.253840</td>\n",
       "      <td>-0.590251</td>\n",
       "      <td>-0.649230</td>\n",
       "      <td>-0.156709</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017741</td>\n",
       "      <td>-0.069778</td>\n",
       "      <td>-0.224950</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>1.350797</td>\n",
       "      <td>0.169538</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.875875</td>\n",
       "      <td>-0.588526</td>\n",
       "      <td>-0.224950</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>-0.754215</td>\n",
       "      <td>2.344518</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PAYSTAT_SEP  CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  \\\n",
       "0     1.804974    -0.652117 -0.344828 -1.130262     -0.651099 -1.244199   \n",
       "1    -0.875875    -0.668758 -0.344828 -0.358818     -0.999086 -1.026701   \n",
       "2     0.017741    -0.309746 -0.253840 -0.590251     -0.649230 -0.156709   \n",
       "3     0.017741    -0.069778 -0.224950 -0.898829      1.350797  0.169538   \n",
       "4    -0.875875    -0.588526 -0.224950 -0.898829     -0.754215  2.344518   \n",
       "\n",
       "   No_payment  Paid_in_time  Paid_partly  1_month_late  2_months_late  \\\n",
       "0    -0.31168     -0.488987    -1.000272     -0.355373       3.169916   \n",
       "1    -0.31168      2.045043    -1.000272     -0.355373      -0.315466   \n",
       "2    -0.31168     -0.488987     0.999729     -0.355373      -0.315466   \n",
       "3    -0.31168     -0.488987     0.999729     -0.355373      -0.315466   \n",
       "4    -0.31168      2.045043    -1.000272     -0.355373      -0.315466   \n",
       "\n",
       "   3_months_late  4_months_late  5_months_late  6_months_late  7_months_late  \\\n",
       "0      -0.105112      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "1      -0.105112      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "2      -0.105112      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "3      -0.105112      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "4      -0.105112      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "\n",
       "   8_months_late  \n",
       "0      -0.025401  \n",
       "1      -0.025401  \n",
       "2      -0.025401  \n",
       "3      -0.025401  \n",
       "4      -0.025401  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the features that will function as independent variables:\n",
    "credit_indep = credit.iloc[:, 0:(len(credit.columns)-1)]\n",
    "credit_indep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: DEFAULT, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the dependent variable and turn it into a seperate vector:\n",
    "credit_dep = credit['DEFAULT']\n",
    "credit_dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAYSTAT_SEP</th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>29466.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.208420e-15</td>\n",
       "      <td>-2.103345e-16</td>\n",
       "      <td>2.656975e-16</td>\n",
       "      <td>-3.281235e-15</td>\n",
       "      <td>3.921537e-16</td>\n",
       "      <td>4.803280e-16</td>\n",
       "      <td>1.062419e-14</td>\n",
       "      <td>2.150282e-15</td>\n",
       "      <td>1.553156e-15</td>\n",
       "      <td>1.034401e-15</td>\n",
       "      <td>-2.365040e-15</td>\n",
       "      <td>5.959323e-15</td>\n",
       "      <td>-7.981141e-16</td>\n",
       "      <td>-7.367930e-16</td>\n",
       "      <td>-4.664973e-16</td>\n",
       "      <td>-8.970036e-16</td>\n",
       "      <td>-8.155870e-15</td>\n",
       "      <td>0.792914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>0.405225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.769491e+00</td>\n",
       "      <td>-2.943419e+00</td>\n",
       "      <td>-3.448283e-01</td>\n",
       "      <td>-1.207406e+00</td>\n",
       "      <td>-4.485552e+00</td>\n",
       "      <td>-1.570446e+00</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>-1.000272e+00</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.758749e-01</td>\n",
       "      <td>-6.490855e-01</td>\n",
       "      <td>-2.847691e-01</td>\n",
       "      <td>-8.988289e-01</td>\n",
       "      <td>-9.815463e-01</td>\n",
       "      <td>-8.092031e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>-1.000272e+00</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.774131e-02</td>\n",
       "      <td>-3.848821e-01</td>\n",
       "      <td>-2.132614e-01</td>\n",
       "      <td>-2.045296e-01</td>\n",
       "      <td>-2.549452e-01</td>\n",
       "      <td>-1.567091e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.774131e-02</td>\n",
       "      <td>2.204590e-01</td>\n",
       "      <td>-4.252409e-02</td>\n",
       "      <td>5.669140e-01</td>\n",
       "      <td>9.783657e-01</td>\n",
       "      <td>6.045339e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.166671e+00</td>\n",
       "      <td>1.233379e+01</td>\n",
       "      <td>5.201533e+01</td>\n",
       "      <td>6.429886e+00</td>\n",
       "      <td>1.474319e+01</td>\n",
       "      <td>4.736996e+00</td>\n",
       "      <td>3.208424e+00</td>\n",
       "      <td>2.045043e+00</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>2.813944e+00</td>\n",
       "      <td>3.169916e+00</td>\n",
       "      <td>9.513638e+00</td>\n",
       "      <td>1.966496e+01</td>\n",
       "      <td>3.364978e+01</td>\n",
       "      <td>5.174676e+01</td>\n",
       "      <td>5.721014e+01</td>\n",
       "      <td>3.936803e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PAYSTAT_SEP   CUR_BIL_SEP      PAID_SEP        CREDIT  CREDIT_RATIO  \\\n",
       "count  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04   \n",
       "mean  -3.208420e-15 -2.103345e-16  2.656975e-16 -3.281235e-15  3.921537e-16   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -1.769491e+00 -2.943419e+00 -3.448283e-01 -1.207406e+00 -4.485552e+00   \n",
       "25%   -8.758749e-01 -6.490855e-01 -2.847691e-01 -8.988289e-01 -9.815463e-01   \n",
       "50%    1.774131e-02 -3.848821e-01 -2.132614e-01 -2.045296e-01 -2.549452e-01   \n",
       "75%    1.774131e-02  2.204590e-01 -4.252409e-02  5.669140e-01  9.783657e-01   \n",
       "max    7.166671e+00  1.233379e+01  5.201533e+01  6.429886e+00  1.474319e+01   \n",
       "\n",
       "                AGE    No_payment  Paid_in_time   Paid_partly  1_month_late  \\\n",
       "count  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04   \n",
       "mean   4.803280e-16  1.062419e-14  2.150282e-15  1.553156e-15  1.034401e-15   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -1.570446e+00 -3.116795e-01 -4.889872e-01 -1.000272e+00 -3.553731e-01   \n",
       "25%   -8.092031e-01 -3.116795e-01 -4.889872e-01 -1.000272e+00 -3.553731e-01   \n",
       "50%   -1.567091e-01 -3.116795e-01 -4.889872e-01  9.997285e-01 -3.553731e-01   \n",
       "75%    6.045339e-01 -3.116795e-01 -4.889872e-01  9.997285e-01 -3.553731e-01   \n",
       "max    4.736996e+00  3.208424e+00  2.045043e+00  9.997285e-01  2.813944e+00   \n",
       "\n",
       "       2_months_late  3_months_late  4_months_late  5_months_late  \\\n",
       "count   2.946600e+04   2.946600e+04   2.946600e+04   2.946600e+04   \n",
       "mean   -2.365040e-15   5.959323e-15  -7.981141e-16  -7.367930e-16   \n",
       "std     1.000017e+00   1.000017e+00   1.000017e+00   1.000017e+00   \n",
       "min    -3.154657e-01  -1.051123e-01  -5.085188e-02  -2.971788e-02   \n",
       "25%    -3.154657e-01  -1.051123e-01  -5.085188e-02  -2.971788e-02   \n",
       "50%    -3.154657e-01  -1.051123e-01  -5.085188e-02  -2.971788e-02   \n",
       "75%    -3.154657e-01  -1.051123e-01  -5.085188e-02  -2.971788e-02   \n",
       "max     3.169916e+00   9.513638e+00   1.966496e+01   3.364978e+01   \n",
       "\n",
       "       6_months_late  7_months_late  8_months_late       DEFAULT  \n",
       "count   2.946600e+04   2.946600e+04   2.946600e+04  29466.000000  \n",
       "mean   -4.664973e-16  -8.970036e-16  -8.155870e-15      0.792914  \n",
       "std     1.000017e+00   1.000017e+00   1.000017e+00      0.405225  \n",
       "min    -1.932488e-02  -1.747942e-02  -2.540132e-02      0.000000  \n",
       "25%    -1.932488e-02  -1.747942e-02  -2.540132e-02      1.000000  \n",
       "50%    -1.932488e-02  -1.747942e-02  -2.540132e-02      1.000000  \n",
       "75%    -1.932488e-02  -1.747942e-02  -2.540132e-02      1.000000  \n",
       "max     5.174676e+01   5.721014e+01   3.936803e+01      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20626, 17) (20626,)\n",
      "(8840, 17) (8840,)\n",
      "[ 4285 16341]\n"
     ]
    }
   ],
   "source": [
    "#generate a dataframe that functions as a trainingset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit_indep, credit_dep, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance the data\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32682, 17) (32682,)\n",
      "(8840, 17) (8840,)\n",
      "[16341 16341]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011761535265517\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for k = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.801909237570136, 0.818555429538215, 0.7900074487477495, 0.8011761535265517, 0.7797877855509265, 0.7887840200030694, 0.7756880172481555, 0.7793905127658062]\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data to select optimal K \n",
    "k_range = range(2,10)\n",
    "k_scores= []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "    k_scores.append(CVscores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross validation score')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Cross validation score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6940045248868778\n",
      "Cohen's Kappa = 0.23387161369407794\n",
      "Recall = 0.7296027338744127\n",
      "F1 score = 0.7911680691731645\n",
      "ROC area under curve = 0.6430072007291711\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the top K that was retrieved from the Knn cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors = k_range[((k_scores.index(max(k_scores))))])\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6971730130746109\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for a logistic regression\n",
    "\n",
    "logregression = LogisticRegression(solver='liblinear')\n",
    "CVscores = cross_val_score(logregression, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7854072398190045\n",
      "Cohen's Kappa = 0.3668236730863247\n",
      "Recall = 0.8517727466894489\n",
      "F1 score = 0.863141187504509\n",
      "ROC area under curve = 0.6903332638235357\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the logregression\n",
    "\n",
    "logregression.fit(X_train, y_train)\n",
    "y_pred = logregression.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train using a Support vector machine, a random forest and a logistic regression, \n",
    "\n",
    "modelSVC.fit(X_train, y_train)\n",
    "modelRF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70708647 0.70883055 0.70433266]\n",
      "[0.78134753 0.8366991  0.82825408]\n",
      "0.7079432103298452\n",
      "0.9905758521510312\n"
     ]
    }
   ],
   "source": [
    "#Print cross-validation scores \n",
    "\n",
    "print(cross_val_score(modelSVC, X_train, y_train)) \n",
    "print(cross_val_score(modelRF, X_train, y_train)) \n",
    "\n",
    "#Print model validation scores \n",
    "\n",
    "print(modelSVC.score(X_train, y_train))\n",
    "print(modelRF.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8079185520361991\n",
      "Cohen's Kappa = 0.4065312121157727\n",
      "Recall = 0.8822440552470454\n",
      "F1 score = 0.8794889992902768\n",
      "ROC area under curve = 0.7014412351083879\n",
      "[[ 946  871]\n",
      " [ 827 6196]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy of using the Support Vector Machine\n",
    "y_pred = modelSVC.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7414027149321267\n",
      "Cohen's Kappa = 0.29376255899832904\n",
      "Recall = 0.7949594190516873\n",
      "F1 score = 0.8300624442462088\n",
      "ROC area under curve = 0.6646783886672856\n",
      "[[ 971  846]\n",
      " [1440 5583]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification of accuracy using the Random forest\n",
    "y_pred = modelRF.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim=17, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "32682/32682 [==============================] - 1s 45us/step - loss: 0.7149 - acc: 0.5494\n",
      "Epoch 2/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.6524 - acc: 0.6780\n",
      "Epoch 3/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.6210 - acc: 0.6988\n",
      "Epoch 4/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5913 - acc: 0.7012\n",
      "Epoch 5/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5711 - acc: 0.7066\n",
      "Epoch 6/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5623 - acc: 0.7065\n",
      "Epoch 7/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5602 - acc: 0.7082\n",
      "Epoch 8/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5591 - acc: 0.7077\n",
      "Epoch 9/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5586 - acc: 0.7080\n",
      "Epoch 10/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5577 - acc: 0.7084\n",
      "Epoch 11/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5572 - acc: 0.7086\n",
      "Epoch 12/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5566 - acc: 0.7088\n",
      "Epoch 13/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5562 - acc: 0.7108\n",
      "Epoch 14/150\n",
      "32682/32682 [==============================] - 1s 17us/step - loss: 0.5558 - acc: 0.7088\n",
      "Epoch 15/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5552 - acc: 0.7100\n",
      "Epoch 16/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5549 - acc: 0.7109\n",
      "Epoch 17/150\n",
      "32682/32682 [==============================] - 1s 19us/step - loss: 0.5546 - acc: 0.7101\n",
      "Epoch 18/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5542 - acc: 0.7104\n",
      "Epoch 19/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5538 - acc: 0.7118\n",
      "Epoch 20/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5536 - acc: 0.7098\n",
      "Epoch 21/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5533 - acc: 0.7115\n",
      "Epoch 22/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5532 - acc: 0.7112\n",
      "Epoch 23/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5531 - acc: 0.7115\n",
      "Epoch 24/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5528 - acc: 0.7111\n",
      "Epoch 25/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5526 - acc: 0.7104\n",
      "Epoch 26/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5524 - acc: 0.7112\n",
      "Epoch 27/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5525 - acc: 0.7120\n",
      "Epoch 28/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5523 - acc: 0.7121\n",
      "Epoch 29/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5521 - acc: 0.7104\n",
      "Epoch 30/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5521 - acc: 0.7103\n",
      "Epoch 31/150\n",
      "32682/32682 [==============================] - 1s 17us/step - loss: 0.5522 - acc: 0.7111\n",
      "Epoch 32/150\n",
      "32682/32682 [==============================] - 1s 19us/step - loss: 0.5519 - acc: 0.7132\n",
      "Epoch 33/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5518 - acc: 0.7105\n",
      "Epoch 34/150\n",
      "32682/32682 [==============================] - 1s 17us/step - loss: 0.5517 - acc: 0.7109\n",
      "Epoch 35/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5517 - acc: 0.7119\n",
      "Epoch 36/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5516 - acc: 0.7098: 0s - loss: 0.5505 - acc: \n",
      "Epoch 37/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5514 - acc: 0.7119\n",
      "Epoch 38/150\n",
      "32682/32682 [==============================] - 1s 23us/step - loss: 0.5513 - acc: 0.7115\n",
      "Epoch 39/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5513 - acc: 0.7125\n",
      "Epoch 40/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5513 - acc: 0.7120\n",
      "Epoch 41/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5516 - acc: 0.7113\n",
      "Epoch 42/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5511 - acc: 0.7129\n",
      "Epoch 43/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5509 - acc: 0.7123\n",
      "Epoch 44/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5508 - acc: 0.7130\n",
      "Epoch 45/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5509 - acc: 0.7130\n",
      "Epoch 46/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5506 - acc: 0.7127\n",
      "Epoch 47/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5507 - acc: 0.7129\n",
      "Epoch 48/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5506 - acc: 0.7142\n",
      "Epoch 49/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5506 - acc: 0.7115\n",
      "Epoch 50/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5502 - acc: 0.7118\n",
      "Epoch 51/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5501 - acc: 0.7131\n",
      "Epoch 52/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5505 - acc: 0.7142\n",
      "Epoch 53/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5500 - acc: 0.7146\n",
      "Epoch 54/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5498 - acc: 0.7135\n",
      "Epoch 55/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5499 - acc: 0.7125\n",
      "Epoch 56/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5496 - acc: 0.7143\n",
      "Epoch 57/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5497 - acc: 0.7140\n",
      "Epoch 58/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5494 - acc: 0.7160\n",
      "Epoch 59/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5497 - acc: 0.7151\n",
      "Epoch 60/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5493 - acc: 0.7135\n",
      "Epoch 61/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5493 - acc: 0.7151\n",
      "Epoch 62/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5491 - acc: 0.7140\n",
      "Epoch 63/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5491 - acc: 0.7132\n",
      "Epoch 64/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5489 - acc: 0.7136\n",
      "Epoch 65/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5486 - acc: 0.7152\n",
      "Epoch 66/150\n",
      "32682/32682 [==============================] - 1s 15us/step - loss: 0.5488 - acc: 0.7146\n",
      "Epoch 67/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5485 - acc: 0.7166\n",
      "Epoch 68/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5485 - acc: 0.7155\n",
      "Epoch 69/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5484 - acc: 0.7156\n",
      "Epoch 70/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5483 - acc: 0.7156\n",
      "Epoch 71/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5484 - acc: 0.7144\n",
      "Epoch 72/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5483 - acc: 0.7161\n",
      "Epoch 73/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5480 - acc: 0.7147\n",
      "Epoch 74/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5478 - acc: 0.7153\n",
      "Epoch 75/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5475 - acc: 0.7153\n",
      "Epoch 76/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5475 - acc: 0.7157\n",
      "Epoch 77/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5474 - acc: 0.7170\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5474 - acc: 0.7168\n",
      "Epoch 79/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5471 - acc: 0.7151\n",
      "Epoch 80/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5473 - acc: 0.7146\n",
      "Epoch 81/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5471 - acc: 0.7157\n",
      "Epoch 82/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5470 - acc: 0.7165\n",
      "Epoch 83/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5473 - acc: 0.7160\n",
      "Epoch 84/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5467 - acc: 0.7153: 0s - loss: 0.5515 - ac\n",
      "Epoch 85/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5465 - acc: 0.7158\n",
      "Epoch 86/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5466 - acc: 0.7162\n",
      "Epoch 87/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5466 - acc: 0.7155\n",
      "Epoch 88/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5463 - acc: 0.7161\n",
      "Epoch 89/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5462 - acc: 0.7159\n",
      "Epoch 90/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5462 - acc: 0.7153\n",
      "Epoch 91/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5460 - acc: 0.7173\n",
      "Epoch 92/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5461 - acc: 0.7157: 0s - loss: 0.5456 - acc:\n",
      "Epoch 93/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5458 - acc: 0.7156\n",
      "Epoch 94/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5457 - acc: 0.7164\n",
      "Epoch 95/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5456 - acc: 0.7168\n",
      "Epoch 96/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5456 - acc: 0.7158\n",
      "Epoch 97/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5457 - acc: 0.7164\n",
      "Epoch 98/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.7159\n",
      "Epoch 99/150\n",
      "32682/32682 [==============================] - 1s 17us/step - loss: 0.5455 - acc: 0.7160\n",
      "Epoch 100/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5454 - acc: 0.7168\n",
      "Epoch 101/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5453 - acc: 0.7166\n",
      "Epoch 102/150\n",
      "32682/32682 [==============================] - 1s 18us/step - loss: 0.5452 - acc: 0.7172\n",
      "Epoch 103/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5453 - acc: 0.7165\n",
      "Epoch 104/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5451 - acc: 0.7170\n",
      "Epoch 105/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5453 - acc: 0.7169\n",
      "Epoch 106/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5448 - acc: 0.7186\n",
      "Epoch 107/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5449 - acc: 0.7177\n",
      "Epoch 108/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5449 - acc: 0.7171\n",
      "Epoch 109/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5448 - acc: 0.7175\n",
      "Epoch 110/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5447 - acc: 0.7169\n",
      "Epoch 111/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5450 - acc: 0.7173\n",
      "Epoch 112/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5447 - acc: 0.7170\n",
      "Epoch 113/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5447 - acc: 0.7161\n",
      "Epoch 114/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5445 - acc: 0.7172\n",
      "Epoch 115/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5447 - acc: 0.7178\n",
      "Epoch 116/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5444 - acc: 0.7174\n",
      "Epoch 117/150\n",
      "32682/32682 [==============================] - 1s 16us/step - loss: 0.5444 - acc: 0.7180\n",
      "Epoch 118/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5442 - acc: 0.7173\n",
      "Epoch 119/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5441 - acc: 0.7174\n",
      "Epoch 120/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5443 - acc: 0.7184\n",
      "Epoch 121/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5443 - acc: 0.7176\n",
      "Epoch 122/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5438 - acc: 0.7183\n",
      "Epoch 123/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5438 - acc: 0.7188\n",
      "Epoch 124/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5438 - acc: 0.7195\n",
      "Epoch 125/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5437 - acc: 0.7193\n",
      "Epoch 126/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5441 - acc: 0.7171\n",
      "Epoch 127/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5437 - acc: 0.7179\n",
      "Epoch 128/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5436 - acc: 0.7179\n",
      "Epoch 129/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5433 - acc: 0.7187\n",
      "Epoch 130/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5433 - acc: 0.7178\n",
      "Epoch 131/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5433 - acc: 0.7171\n",
      "Epoch 132/150\n",
      "32682/32682 [==============================] - 1s 15us/step - loss: 0.5434 - acc: 0.7192\n",
      "Epoch 133/150\n",
      "32682/32682 [==============================] - 1s 17us/step - loss: 0.5429 - acc: 0.7176\n",
      "Epoch 134/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5434 - acc: 0.7188\n",
      "Epoch 135/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5428 - acc: 0.7189\n",
      "Epoch 136/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5433 - acc: 0.7176\n",
      "Epoch 137/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5427 - acc: 0.7185\n",
      "Epoch 138/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5430 - acc: 0.7177\n",
      "Epoch 139/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5427 - acc: 0.7205\n",
      "Epoch 140/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5425 - acc: 0.7186\n",
      "Epoch 141/150\n",
      "32682/32682 [==============================] - 0s 15us/step - loss: 0.5428 - acc: 0.7184\n",
      "Epoch 142/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5426 - acc: 0.7197\n",
      "Epoch 143/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5424 - acc: 0.7198\n",
      "Epoch 144/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5424 - acc: 0.7173\n",
      "Epoch 145/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5424 - acc: 0.7180\n",
      "Epoch 146/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5425 - acc: 0.7190\n",
      "Epoch 147/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5423 - acc: 0.7186\n",
      "Epoch 148/150\n",
      "32682/32682 [==============================] - 0s 14us/step - loss: 0.5423 - acc: 0.7186\n",
      "Epoch 149/150\n",
      "32682/32682 [==============================] - 0s 12us/step - loss: 0.5420 - acc: 0.7197\n",
      "Epoch 150/150\n",
      "32682/32682 [==============================] - 0s 13us/step - loss: 0.5421 - acc: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d759833c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the neural network\n",
    "model.fit(X_train, y_train, epochs=150, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7529411764705882\n",
      "Cohen's Kappa = 0.3503328564220727\n",
      "Recall = 0.7871280079738003\n",
      "F1 score = 0.8350453172205438\n",
      "ROC area under curve = 0.7039657651316442\n"
     ]
    }
   ],
   "source": [
    "#make predictions and round answers\n",
    "y_pred = model.predict(X_test)\n",
    "rounded = [round(x[0]) for x in y_pred]\n",
    "y_pred = np.array(rounded, dtype = 'int64')\n",
    "\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best results result from the SVM\n",
    "\n",
    "Accuracy = 0.8079185520361991\n",
    "\n",
    "Cohen's Kappa = 0.4065312121157727\n",
    "\n",
    "Recall = 0.8822440552470454\n",
    "\n",
    "F1 score = 0.8794889992902768\n",
    "\n",
    "ROC area under curve = 0.7014412351083879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
