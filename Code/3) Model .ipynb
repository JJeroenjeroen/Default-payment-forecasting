{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whatever will be used in this notebook\n",
    "import pylab\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#import SMOTE to balance trainset \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import from sklearn\n",
    "#Estimators:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Set generation \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Performance metrics:\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#K-fold crossvalidation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings regarding the version of the pandas library which is used \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make variable for the imported models\n",
    "modelSVC = SVC()\n",
    "modelRF = RandomForestClassifier()\n",
    "modelLR = LogisticRegression()\n",
    "modelKNN = KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv file and remove the extra ID column\n",
    "credit = pd.read_csv(\"C:\\\\Users\\\\Jeroen\\\\Desktop\\\\Ubiqum\\\\Data Science\\\\Excel Files\\\\credit_3.csv\", header = 0)\n",
    "credit = credit[credit.columns[1:len(credit.columns)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29352</th>\n",
       "      <td>0.272772</td>\n",
       "      <td>0.074222</td>\n",
       "      <td>-0.666226</td>\n",
       "      <td>1.364668</td>\n",
       "      <td>-0.155550</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>3.163463</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29353</th>\n",
       "      <td>1.845358</td>\n",
       "      <td>0.164042</td>\n",
       "      <td>0.414161</td>\n",
       "      <td>1.129704</td>\n",
       "      <td>0.388520</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>0.996022</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29354</th>\n",
       "      <td>-0.684823</td>\n",
       "      <td>-0.234941</td>\n",
       "      <td>-0.126033</td>\n",
       "      <td>-1.009681</td>\n",
       "      <td>0.823777</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>2.040573</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29355</th>\n",
       "      <td>-0.659395</td>\n",
       "      <td>-0.344942</td>\n",
       "      <td>-1.052079</td>\n",
       "      <td>-0.762930</td>\n",
       "      <td>0.170892</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>19.628457</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29356</th>\n",
       "      <td>-0.059983</td>\n",
       "      <td>-0.220510</td>\n",
       "      <td>-0.897738</td>\n",
       "      <td>1.381377</td>\n",
       "      <td>1.150219</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>0.996022</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  No_payment  \\\n",
       "29352     0.272772  0.074222 -0.666226      1.364668 -0.155550   -0.310141   \n",
       "29353     1.845358  0.164042  0.414161      1.129704  0.388520   -0.310141   \n",
       "29354    -0.684823 -0.234941 -0.126033     -1.009681  0.823777   -0.310141   \n",
       "29355    -0.659395 -0.344942 -1.052079     -0.762930  0.170892   -0.310141   \n",
       "29356    -0.059983 -0.220510 -0.897738      1.381377  1.150219   -0.310141   \n",
       "\n",
       "       Paid_in_time  Paid_partly  1_month_late  2_months_late  3_months_late  \\\n",
       "29352     -0.490058    -1.003993     -0.351545       3.163463      -0.105309   \n",
       "29353     -0.490058     0.996022     -0.351545      -0.316109      -0.105309   \n",
       "29354      2.040573    -1.003993     -0.351545      -0.316109      -0.105309   \n",
       "29355     -0.490058    -1.003993     -0.351545      -0.316109      -0.105309   \n",
       "29356     -0.490058     0.996022     -0.351545      -0.316109      -0.105309   \n",
       "\n",
       "       4_months_late  5_months_late  6_months_late  7_months_late  \\\n",
       "29352      -0.050946      -0.029773      -0.019361      -0.017512   \n",
       "29353      -0.050946      -0.029773      -0.019361      -0.017512   \n",
       "29354      -0.050946      -0.029773      -0.019361      -0.017512   \n",
       "29355      19.628457      -0.029773      -0.019361      -0.017512   \n",
       "29356      -0.050946      -0.029773      -0.019361      -0.017512   \n",
       "\n",
       "       8_months_late  DEFAULT  \n",
       "29352      -0.025448        0  \n",
       "29353      -0.025448        1  \n",
       "29354      -0.025448        1  \n",
       "29355      -0.025448        0  \n",
       "29356      -0.025448        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give head of the df so it is easily visible which vars should function as independent variable\n",
    "credit.head()\n",
    "credit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.654693</td>\n",
       "      <td>-0.344942</td>\n",
       "      <td>-1.129250</td>\n",
       "      <td>-0.655908</td>\n",
       "      <td>-1.243692</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>3.163463</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.671325</td>\n",
       "      <td>-0.344942</td>\n",
       "      <td>-0.357544</td>\n",
       "      <td>-1.004264</td>\n",
       "      <td>-1.026063</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>2.040573</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.312508</td>\n",
       "      <td>-0.254043</td>\n",
       "      <td>-0.589056</td>\n",
       "      <td>-0.654038</td>\n",
       "      <td>-0.155550</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>0.996022</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.072670</td>\n",
       "      <td>-0.225181</td>\n",
       "      <td>-0.897738</td>\n",
       "      <td>1.348107</td>\n",
       "      <td>0.170892</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>-0.490058</td>\n",
       "      <td>0.996022</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.591136</td>\n",
       "      <td>-0.225181</td>\n",
       "      <td>-0.897738</td>\n",
       "      <td>-0.759134</td>\n",
       "      <td>2.347174</td>\n",
       "      <td>-0.310141</td>\n",
       "      <td>2.040573</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>-0.351545</td>\n",
       "      <td>-0.316109</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.050946</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.019361</td>\n",
       "      <td>-0.017512</td>\n",
       "      <td>-0.025448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  No_payment  \\\n",
       "0    -0.654693 -0.344942 -1.129250     -0.655908 -1.243692   -0.310141   \n",
       "1    -0.671325 -0.344942 -0.357544     -1.004264 -1.026063   -0.310141   \n",
       "2    -0.312508 -0.254043 -0.589056     -0.654038 -0.155550   -0.310141   \n",
       "3    -0.072670 -0.225181 -0.897738      1.348107  0.170892   -0.310141   \n",
       "4    -0.591136 -0.225181 -0.897738     -0.759134  2.347174   -0.310141   \n",
       "\n",
       "   Paid_in_time  Paid_partly  1_month_late  2_months_late  3_months_late  \\\n",
       "0     -0.490058    -1.003993     -0.351545       3.163463      -0.105309   \n",
       "1      2.040573    -1.003993     -0.351545      -0.316109      -0.105309   \n",
       "2     -0.490058     0.996022     -0.351545      -0.316109      -0.105309   \n",
       "3     -0.490058     0.996022     -0.351545      -0.316109      -0.105309   \n",
       "4      2.040573    -1.003993     -0.351545      -0.316109      -0.105309   \n",
       "\n",
       "   4_months_late  5_months_late  6_months_late  7_months_late  8_months_late  \n",
       "0      -0.050946      -0.029773      -0.019361      -0.017512      -0.025448  \n",
       "1      -0.050946      -0.029773      -0.019361      -0.017512      -0.025448  \n",
       "2      -0.050946      -0.029773      -0.019361      -0.017512      -0.025448  \n",
       "3      -0.050946      -0.029773      -0.019361      -0.017512      -0.025448  \n",
       "4      -0.050946      -0.029773      -0.019361      -0.017512      -0.025448  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the features that will function as independent variables:\n",
    "credit_indep = credit.iloc[:, 0:(len(credit.columns)-1)]\n",
    "credit_indep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: DEFAULT, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the dependent variable and turn it into a seperate vector:\n",
    "credit_dep = credit['DEFAULT']\n",
    "credit_dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>2.935700e+04</td>\n",
       "      <td>29357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.680542e-16</td>\n",
       "      <td>-9.282901e-17</td>\n",
       "      <td>-9.166894e-16</td>\n",
       "      <td>-6.287621e-17</td>\n",
       "      <td>-5.961478e-16</td>\n",
       "      <td>9.799491e-15</td>\n",
       "      <td>2.843921e-15</td>\n",
       "      <td>-1.134771e-15</td>\n",
       "      <td>4.210641e-15</td>\n",
       "      <td>-2.548827e-15</td>\n",
       "      <td>1.608660e-15</td>\n",
       "      <td>3.556643e-15</td>\n",
       "      <td>-1.537443e-15</td>\n",
       "      <td>-7.809724e-16</td>\n",
       "      <td>9.628281e-16</td>\n",
       "      <td>5.406908e-16</td>\n",
       "      <td>0.795858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>0.403080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.075626e-01</td>\n",
       "      <td>-3.449416e-01</td>\n",
       "      <td>-1.206420e+00</td>\n",
       "      <td>-1.039849e+00</td>\n",
       "      <td>-1.570134e+00</td>\n",
       "      <td>-3.101413e-01</td>\n",
       "      <td>-4.900583e-01</td>\n",
       "      <td>-1.003993e+00</td>\n",
       "      <td>-3.515450e-01</td>\n",
       "      <td>-3.161093e-01</td>\n",
       "      <td>-1.053094e-01</td>\n",
       "      <td>-5.094644e-02</td>\n",
       "      <td>-2.977304e-02</td>\n",
       "      <td>-1.936074e-02</td>\n",
       "      <td>-1.751185e-02</td>\n",
       "      <td>-2.544846e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.496534e-01</td>\n",
       "      <td>-2.840431e-01</td>\n",
       "      <td>-8.977380e-01</td>\n",
       "      <td>-9.850933e-01</td>\n",
       "      <td>-8.084351e-01</td>\n",
       "      <td>-3.101413e-01</td>\n",
       "      <td>-4.900583e-01</td>\n",
       "      <td>-1.003993e+00</td>\n",
       "      <td>-3.515450e-01</td>\n",
       "      <td>-3.161093e-01</td>\n",
       "      <td>-1.053094e-01</td>\n",
       "      <td>-5.094644e-02</td>\n",
       "      <td>-2.977304e-02</td>\n",
       "      <td>-1.936074e-02</td>\n",
       "      <td>-1.751185e-02</td>\n",
       "      <td>-2.544846e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.845766e-01</td>\n",
       "      <td>-2.132046e-01</td>\n",
       "      <td>-2.032031e-01</td>\n",
       "      <td>-2.504315e-01</td>\n",
       "      <td>-1.555504e-01</td>\n",
       "      <td>-3.101413e-01</td>\n",
       "      <td>-4.900583e-01</td>\n",
       "      <td>9.960225e-01</td>\n",
       "      <td>-3.515450e-01</td>\n",
       "      <td>-3.161093e-01</td>\n",
       "      <td>-1.053094e-01</td>\n",
       "      <td>-5.094644e-02</td>\n",
       "      <td>-2.977304e-02</td>\n",
       "      <td>-1.936074e-02</td>\n",
       "      <td>-1.751185e-02</td>\n",
       "      <td>-2.544846e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.205104e-01</td>\n",
       "      <td>-4.218604e-02</td>\n",
       "      <td>5.685023e-01</td>\n",
       "      <td>9.787472e-01</td>\n",
       "      <td>6.061484e-01</td>\n",
       "      <td>-3.101413e-01</td>\n",
       "      <td>-4.900583e-01</td>\n",
       "      <td>9.960225e-01</td>\n",
       "      <td>-3.515450e-01</td>\n",
       "      <td>-3.161093e-01</td>\n",
       "      <td>-1.053094e-01</td>\n",
       "      <td>-5.094644e-02</td>\n",
       "      <td>-2.977304e-02</td>\n",
       "      <td>-1.936074e-02</td>\n",
       "      <td>-1.751185e-02</td>\n",
       "      <td>-2.544846e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.232417e+01</td>\n",
       "      <td>5.196374e+01</td>\n",
       "      <td>6.433464e+00</td>\n",
       "      <td>1.475468e+01</td>\n",
       "      <td>4.741085e+00</td>\n",
       "      <td>3.224337e+00</td>\n",
       "      <td>2.040573e+00</td>\n",
       "      <td>9.960225e-01</td>\n",
       "      <td>2.844586e+00</td>\n",
       "      <td>3.163463e+00</td>\n",
       "      <td>9.495831e+00</td>\n",
       "      <td>1.962846e+01</td>\n",
       "      <td>3.358743e+01</td>\n",
       "      <td>5.165093e+01</td>\n",
       "      <td>5.710419e+01</td>\n",
       "      <td>3.929510e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUR_BIL_SEP      PAID_SEP        CREDIT  CREDIT_RATIO           AGE  \\\n",
       "count  2.935700e+04  2.935700e+04  2.935700e+04  2.935700e+04  2.935700e+04   \n",
       "mean   2.680542e-16 -9.282901e-17 -9.166894e-16 -6.287621e-17 -5.961478e-16   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -7.075626e-01 -3.449416e-01 -1.206420e+00 -1.039849e+00 -1.570134e+00   \n",
       "25%   -6.496534e-01 -2.840431e-01 -8.977380e-01 -9.850933e-01 -8.084351e-01   \n",
       "50%   -3.845766e-01 -2.132046e-01 -2.032031e-01 -2.504315e-01 -1.555504e-01   \n",
       "75%    2.205104e-01 -4.218604e-02  5.685023e-01  9.787472e-01  6.061484e-01   \n",
       "max    1.232417e+01  5.196374e+01  6.433464e+00  1.475468e+01  4.741085e+00   \n",
       "\n",
       "         No_payment  Paid_in_time   Paid_partly  1_month_late  2_months_late  \\\n",
       "count  2.935700e+04  2.935700e+04  2.935700e+04  2.935700e+04   2.935700e+04   \n",
       "mean   9.799491e-15  2.843921e-15 -1.134771e-15  4.210641e-15  -2.548827e-15   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   1.000017e+00   \n",
       "min   -3.101413e-01 -4.900583e-01 -1.003993e+00 -3.515450e-01  -3.161093e-01   \n",
       "25%   -3.101413e-01 -4.900583e-01 -1.003993e+00 -3.515450e-01  -3.161093e-01   \n",
       "50%   -3.101413e-01 -4.900583e-01  9.960225e-01 -3.515450e-01  -3.161093e-01   \n",
       "75%   -3.101413e-01 -4.900583e-01  9.960225e-01 -3.515450e-01  -3.161093e-01   \n",
       "max    3.224337e+00  2.040573e+00  9.960225e-01  2.844586e+00   3.163463e+00   \n",
       "\n",
       "       3_months_late  4_months_late  5_months_late  6_months_late  \\\n",
       "count   2.935700e+04   2.935700e+04   2.935700e+04   2.935700e+04   \n",
       "mean    1.608660e-15   3.556643e-15  -1.537443e-15  -7.809724e-16   \n",
       "std     1.000017e+00   1.000017e+00   1.000017e+00   1.000017e+00   \n",
       "min    -1.053094e-01  -5.094644e-02  -2.977304e-02  -1.936074e-02   \n",
       "25%    -1.053094e-01  -5.094644e-02  -2.977304e-02  -1.936074e-02   \n",
       "50%    -1.053094e-01  -5.094644e-02  -2.977304e-02  -1.936074e-02   \n",
       "75%    -1.053094e-01  -5.094644e-02  -2.977304e-02  -1.936074e-02   \n",
       "max     9.495831e+00   1.962846e+01   3.358743e+01   5.165093e+01   \n",
       "\n",
       "       7_months_late  8_months_late       DEFAULT  \n",
       "count   2.935700e+04   2.935700e+04  29357.000000  \n",
       "mean    9.628281e-16   5.406908e-16      0.795858  \n",
       "std     1.000017e+00   1.000017e+00      0.403080  \n",
       "min    -1.751185e-02  -2.544846e-02      0.000000  \n",
       "25%    -1.751185e-02  -2.544846e-02      1.000000  \n",
       "50%    -1.751185e-02  -2.544846e-02      1.000000  \n",
       "75%    -1.751185e-02  -2.544846e-02      1.000000  \n",
       "max     5.710419e+01   3.929510e+01      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20549, 16) (20549,)\n",
      "(8808, 16) (8808,)\n",
      "[ 4205 16344]\n"
     ]
    }
   ],
   "source": [
    "#generate a dataframe that functions as a trainingset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit_indep, credit_dep, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance the data\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32688, 16) (32688,)\n",
      "(8808, 16) (8808,)\n",
      "[16344 16344]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8056194064208955\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for k = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8108176591468002, 0.8249530242290172, 0.7954308482963328, 0.8056194064208955, 0.7852748176179729, 0.7904157262154746, 0.7776889792221111, 0.7839916865986172]\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data to select optimal K \n",
    "k_range = range(2,10)\n",
    "k_scores= []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "    k_scores.append(CVscores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross validation score')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd8FHX+x/HXJ40QCISSAAmB0DsECAiK0i2oBM8GHvazHSKKV/R36qlX9c5ydrHrKcLZwHIqKKAoLfTeeyihJgTSP78/duJFSLIbyGZ2k8/z8dgHu7M7M++g5LPznZnPV1QVY4wxpjwhbgcwxhgT+KxYGGOM8cqKhTHGGK+sWBhjjPHKioUxxhivrFgYY4zxyoqFMcYYr6xYGGOM8cqKhTHGGK/C3A5QWRo3bqxJSUluxzDGmKCyePHiA6oa6+1z1aZYJCUlkZaW5nYMY4wJKiKy3ZfP2TCUMcYYr6xYGGOM8cqKhTHGGK+sWBhjjPHKioUxxhivrFgYY4zxyoqFMcYYr6xYBJl1ezOZsWaf2zGMMTWMFYsgcvBYLte/vpBx7y7h6Il8t+MYY2oQKxZBoqhI+c1/lpORlUteYRFfrdrrdiRjTA1ixSJIvP7DVmatz+ChSzrTqnEdpi3f7XYkY0wNYsUiCCzfeYTHvlzH+Z2bcP3ZSYzsEc+Pmw+yPzPH7WjGmBrCikWAy8rJZ/zkpcTWrcXjV3RHRBiZHI8qTF+e7nY8Y0wNYcUigKkq//fxKnYfOcEzY3oSExUBQJvYunRLqG/FwhhTZaxYBLCpaTv5dHk69wxrR0pSw5+9l5ocz4pdR9mSccyldMaYmsSKRYDauC+LP05fzTltG3HHoLanvH9J93hEbCjKGFM1rFgEoJz8Qu58byl1IsJ46qpkQkPklM80rR9J/9aNmL4sHVV1IaUxpiaxYhGAHv1sDev3ZfHEVT2IqxdZ5udSk+PZciCbVbszqzCdMaYmsmIRYD5fsYf3FuzgtvNaM6hDXLmfvbBLMyJCQ/hkmd1zYYzxLysWAWTnoePc99EKkhNj+M0FHbx+vn5UOIM6xPLp8nQKi2woyhjjP1YsAkR+YRHjJy8FhWfH9CQ81Lf/NKnJCezPymXBloN+TmiMqcmsWASIf369nmU7j/D3y7uT2DDK5/WGdoqjTkQo05bZVVHGGP+xYhEA5mzI4OU5W7jmrBZc3L1ZhdaNDA/lgq5N+WLVHnILCv2U0BhT01mxcNn+zBwmTllGhybRPHRJ59PaxqjkBLJyCpi9PqOS0xljjIcVCxcVFin3TF1Gdl4Bz13Tk8jw0NPaztltGtG4bgTTbSjKGOMnfi0WInKhiKwXkU0icl8p77cQkVkislREVojICGf5cBFZLCIrnT+H+DOnW16as5kfNh3kkZFdaNck+rS3ExYawiXd45m5dh9ZOTYpkjGm8vmtWIhIKPA8cBHQGRgjIiePszwATFXVnsBo4AVn+QHgUlXtBlwPvOOvnG5J23aIJ2ds4NIe8VyVknjG2xuZHE9uQRFfrbYpV40xlc+fRxZ9gU2qukVV84D3gdSTPqNAPed5fSAdQFWXqmrxmMpqIFJEavkxa5U6cjyPCe8vIyGmNn+5rCsip7bzqKieiTEkNqzNNLtBzxjjB/4sFgnAzhKvdznLSnoYGCsiu4AvgPGlbOdyYKmq5vojZFVTVX73wQr2Z+Xw7Jie1IsMr5TtigipPRL4YdMBMrKqxV+VMSaA+LNYlPZ1+eTbjMcAb6pqc2AE8I6I/JRJRLoAjwG3lboDkVtFJE1E0jIyguNKoHfmb+frNfv4/YUd6ZEYU6nbTk2Op0jh8xV2otsYU7n8WSx2ASUH45vjDDOVcDMwFUBV5wGRQGMAEWkOfAxcp6qbS9uBqk5S1RRVTYmNja3k+JVvdfpR/vzZWgZ3iOWmc1pV+vbbNYmmc7N6TLO25caYSubPYrEIaCcirUQkAs8J7OknfWYHMBRARDrhKRYZIhIDfA7cr6o/+DFjlcnOLWD85KU0qBPOP6/sQUgpbccrQ2pyPEt3HGH7wWy/bN8YUzP5rVioagFwJ/AVsBbPVU+rReRRERnpfOxe4BYRWQ5MBm5Qz+QMdwJtgQdFZJnzKL8Fa4B7aNpqth7I5umre9Korv/O1V/aIx7A7rkwxlQqqS4T56SkpGhaWprbMUr10ZJdTJy6nLuGtmPi8PZ+399VL8/j4LFcZk4cWClXWhljqi8RWayqKd4+Z3dw+9mWjGM88Mkq+iY15K4hp06P6g+pyfFszshmzR6bFMkYUzmsWPhRbkEh4ycvJSIshH+NSSbMx7bjZ2pE12aEhYgNRRljKo0VCz/62xfrWJ2eyT+v6EGz+rWrbL8N6kQwsH0s05enU2STIhljKoEVCz+ZsWYfb/64jRvPSWJY5yZVvv/UngnsOZrDom2Hqnzfxpjqx4qFH6QfOcFvP1hO14R63HdRR1cyDOsUR1REKJ/YUJQxphJYsahkBYVFTHh/KfkFRTw7phe1wk6v7fiZiooI4/zOTfhi5R7yCopcyWCMqT6sWFSyZ77ZyKJth/nLZd1o1biOq1lSkxM4eiKf7zYERysUY0zgsmJRiX7cfIBnZ23iit7NGdXz5J6JVW9Au8Y0iAq39h/GmDNmxaKSHDyWy93vL6NV4zo8mtrF7TgAhIeGcHH3ZsxYs5fs3AK34xhjgpgVi0pQVKTc+5/lHDmRz3NjehEVEeZ2pJ+MSk4gJ7+IGWtsUiRjzOmzYlEJXpu7ldnrM3jw4k50jq/nfYUq1KtFAxJibFIkY8yZsWJxhpbvPMJjX67jgi5NGNuvpdtxThESIoxMjue7jQc4eMwmRTLGnB4rFmcgMyefOycvoUm9SB6/vEfANu1LTY6nsEj5YuUet6MYY4KUFYvTpKrc/9FK0o/k8MyYZOpHVc70qP7QsWk9OjSJZprdoGeMOU1WLE7TlEU7+XzFHiYOb0/vlg3djuPVyOR40rYfZueh425HMcYEISsWp2HDviwe/nQ1A9o25o6BbdyO45ORzqRIn9r83MaY02DFooJO5BVy53tLqFsrjCev9t/0qJUtsWEUKS0bWNtyY8xpsWJRQY9+toYN+47x5FXJxEVHuh2nQlKT41m3N4t1e21SJGNMxVixqIDPVqQzeeEO7hjUhvPax7odp8JGdGtGaIjYiW5jTIV5LRbiMVZEHnJetxCRvv6PFlh2HjrO/R+upGeLmCqZR9sfGtWtxbntGjN9mU2KZIypGF+OLF4A+gNjnNdZwPN+SxSA8gqKuHPyUhB4ZnRPwqtoelR/SE2OZ/eREyzZcdjtKMaYIOLLb72zVHUckAOgqoeBCL+mCjBPfL2e5TuP8Pjl3UlsGOV2nDMyvHNTIsNDbCjKGFMhvhSLfBEJBRRARGKBGjObzuz1+3n5uy2M7deCi7o1czvOGatbK4xhnZrw+co95BfWmP+Mxpgz5EuxeAb4GIgTkb8Ac4G/+jVVgNiXmcO9U5fTsWk0D1zc2e04lWZUcgKHsvOYu+mA21GMMUHCay9tVX1XRBYDQwEBRqnqWr8nc1lhkXLPlGUczyvkuWt6EhnuzvSo/nBe+1jq1w5n2tLdDO4Q53YcY0wQKLdYiEgIsEJVuwLrqiZSYHhx9iZ+3HyQx6/oTtu4aLfjVKqIsBBGdGvGtGW7OZ5XEFDzbxhjAlO5w1CqWgQsF5EWVZQnICzadoinZm4kNTmeK3s3dzuOX6Qmx3M8r5CZa/e7HcUYEwR8+UrZDFgtIguB7OKFqjrSb6lcdOR4HhMmL6V5g9r8eVTXgG07fqb6JjWkab1Ipi/b/VPfKGOMKYsvxeIRv6cIEKrKbz9YQcaxXD6842yiIwO37fiZKp4U6fW5WzmcnUeDOjXqamhjTAV5vRpKVefgOV8R7TzWOsuqnbfnbWfGmn3cd1EnujePcTuO343sEU9BkfLfVXvdjmKMCXC+tPu4ClgIXAlcBSwQkSv8HayqrU4/yl8+X8vQjnHcdE6S23GqRJf4erSNq8snNj+3McYLX4ah/gD0UdX98NNNeTOBD/wZrCpl5xYw/r2lNKwTwT+uDNzpUSubiJDaI54nZmwg/cgJ4mNqux3JGBOgfLkpL6S4UDgO+rhe0Hhw2iq2Hczm6dHJNKxhY/cjk51JkZZb+w9jTNl8+aX/pYh8JSI3iMgNwOfAf33ZuIhcKCLrRWSTiNxXyvstRGSWiCwVkRUiMsJZ3shZfkxEnqvID1RRmzOO8enydO4a2o5+rRv5c1cBqWWjOiQnxlivKGNMuXw5wf1b4GWgO9ADmKSqv/O2ntNP6nngIqAzMEZETu6Z8QAwVVV7AqPxdLgFT9PCB4Hf+PhznLY2sXX5/K5zGT+knb93FbBSk+NZsyeTjfuy3I5ijAlQvpzgbgV8oaoTVfUePEcaST5suy+wSVW3qGoe8D6QetJnFKjnPK8PpAOoaraqzsXpdOtv7ZtEExok06P6w8XdmxEiMN2GoowxZfBlGOo//LzLbKGzzJsEYGeJ17ucZSU9DIwVkV3AF8B4H7ZrKllcdCTntG3MtGXpqNqkSMaYU/lSLMKcIwMAnOe+nAUu7av6yb+JxgBvqmpzYATwjtOPyicicquIpIlIWkZGhq+rmVKkJiew49Bxlu484nYUY0wA8uUXc4aI/NTaQ0RSAV96W+8CEku8bo4zzFTCzcBUAFWdB0QCjX3YNs46k1Q1RVVTYmODb07sQHJBlyZEhIUw3U50G2NK4UuxuB34PxHZISI7gd8Dt/mw3iKgnYi0EpEIPCewp5/0mR14Wp8jIp3wFAs7RHBBdGQ4wzrF8dmKdApsUiRjzEl8uRpqs6r2w3NFU2dVPVtVN/mwXgFwJ/AVsBbPVU+rReTREkcq9wK3iMhyYDJwgzqD5iKyDXgSuEFEdpVyJZWpZCN7JHDgWB4/bj7odhRjTIDxege3iEwA3gCygFdEpBdwn6p+7W1dVf0Cz4nrksseKvF8DXBOGesmedu+qVyDOsQSHRnGtGXpnNfehvWMMf/jyzDUTaqaCZwPxAE3An/3ayrjisjwUC7q2pSvVu8lJ7/Q7TjGmADiS7EovqppBPCGqi6n9CudTDUwKjmBY7kFfGOTIhljSvClWCwWka/xFIuvRCSan993YaqRs1o3Ii66FtOsE60xpgRfisXNwH14Os8ex3OPxY1+TWVcExoiXNojntnrMzh6PN/tOMaYAOHL1VBFqrpEVY84rw+q6gr/RzNuSU2OJ6+wiC9X73E7ijEmQFSrVuOmcnRLqE+rxnWsE60x5idWLMwpRISRPeKZt+Ug+zKrpJejMSbA+VQsRCRUROKd+SdaiEgLfwcz7kpNjkfVJkUyxnj40qJ8PLAPmIFn4qPPgc/8nMu4rHVsXbo3r29DUcYYwLcjiwlAB1XtoqrdnEd3fwcz7hvZI56Vu4+yOeOY21GMMS7zpVjsBI76O4gJPJf2iEcE60RrjPHeGwrYAswWkc+B3OKFqvqk31KZgNCkXiT9Wzdi+vJ07h7WDhG7cd+YmsqXI4sdeM5XRADRJR6mBkhNjmfrgWxW7q7+B5cz1uxj8sIdbscwJiB5PbJQ1UcAnDYfqqo2gF2DXNilGQ9+spppy9Lp3jzG7Th+M3XRTn7/kede05SWDWjXxL4PGVOSL1dDdRWRpcAqYLWILBaRLv6PZgJB/ahwBnWI5dPl6RQWVc/5ud9bsIPffbiCs9s0Iio8lKdnbnQ7kjEBx5dhqEnARFVtqaot8UxY9Ip/Y5lAMqpnAvuzcpm/pfpNivT2vG3838crGdIxjteu78NNA1rx+co9rEnPdDuaMQHFl2JRR1VnFb9Q1dlAHb8lMgFnSMc46tYKq3adaF+bu5WHpq1meOcmvDi2F5HhofxqQGuiI8N4auYGt+MZE1B8KRZbRORBEUlyHg8AW/0dzASOyPBQLujSlP+uqj6TIr08ZzN/+mwNF3VtyvPX9KJWWCjgGXa75dzWzFizjxW7jric0pjA4dNMeUAs8BHwsfPcWpTXMKnJ8WTlFDB7fYbbUc7Y87M28bf/ruOS7s14ZkxPIsJ+/s/gxnOSiIkK58kZdnRhTDFfWpQfVtW7VLWXqvZU1QmqergqwpnAcXabRjSuG8H05cE9FPWvmRv5x1frGZUcz9NXJxMeeuo/gejIcG47rw2z12eweLv9r24MlFMsRORp589PRWT6yY+qi2gCQVhoCJd0j2fm2v1k5QTfpEiqyhNfr+epmRu4ondznrgqmbBSCkWx6/q3pFGdCJ6yowtjgPLvs3jH+fOfVRHEBL6RyfG8+eM2vly1lytTEt2O4zNV5bEv1/PSnM2M7pPIXy/rRkhI+Xej16kVxh2D2vDnz9eyYMtBzmrdqIrSGhOYyvxqpaqLnafJqjqn5ANIrpp4JpD0TIyhRcMopgdR23JV5c+fr+WlOZsZ26+FT4Wi2Nh+LYmLrsUTMzagWj3vMTHGV76c4L6+lGU3VHIOEwREhNTkeH7YdID9WYE/KZKq8sina3ht7lZuODuJP6V29blQgOcqsHGD27Jw6yF+2FT97jExpiLKO2cxRkQ+BVqddL5iFmD/cmqo1OR4ihQ+XxHY83MXFSkPfLKKN3/cxi3ntuKPl3Y+rUaIo/sm0qx+JE/OWG9HF6ZGK++cxY/AHqAx8ESJ5VnACn+GMoGrbVw0nZvVY9qydG48p5XbcUpVVKTc/9FKpqTt5I5BbfjdBR1Ou2NurbBQ7hzSlj98vIrZGzIY3CGuktMaExzKO2exXVVnq2r/k85ZLFHVgqoMaQJLanI8y3YeYfvBbLejnKKwSPnNB8uZkraTu4a0PaNCUezK3ok0b1Cbp+zchanBfGkk2E9EFonIMRHJE5FCEbHGOTVYoE6KVFBYxMSpy/hoyW4mDm/PxPPPvFAARISFcNfQdqzYdZQZa/ZVQlJjgo8vJ7ifA8YAG4HawK+AZ/0ZygS2+Jja9E1qyCfLdgfMN+38wiImTFnGtGXp/O7CDtw1tF2lbv8XPRNo1bgOT87YQFE17b5rTHl8KRao6iYgVFULVfUNYLB/Y5lAl5qcwOaMbFYHQHfWvIIixr+3lM9X7OEPIzrx60FtK30fYaEhTBjajnV7s/jvqr2Vvn1jAp0vxeK4iEQAy0TkcRG5B+s6W+Nd1LUp4aHi+j0XuQWF/PrdJXy5ei9/vLQzt5zX2m/7urRHPG3j6vL0zA3Vdm4PY8riS7G4FggF7gSygUTgcn+GMoGvQZ0IBraPZfqydNeGZXLyC7n9ncXMXLuPP43q6vers0JDhLuHtWPj/mN8tiKwztcY42++NBLcrqonVDVTVR9R1YnOsJSp4UYmJ7A3M4eF2w5V+b5z8gu55e00Zm/I4G+/6Ma1/VpWyX5HdG1Gx6bRPD1zIwWFRVWyT2MCQXk35a0UkRVlPXzZuIhcKCLrRWSTiNxXyvstRGSWiCx1tjuixHv3O+utF5ELTu/HM/40rFMcURGhTKviq6KO5xVw05uLmLvpAI9f3p0xfVtU2b5DQoR7hrdn64FsPl4a3B14jamI8o4sLgEuBb50Hr90Hl8AH3jbsIiEAs8DFwGdgTEi0vmkjz0ATFXVnsBo4AVn3c7O6y7AhcALzvZMAImKCOP8zk34YuUe8gqq5lv2sdwCbnhjEfO3HOTJq3q40tDw/M5N6JZQn2e+3Ui+HV2YGsLbTXnbgXNU9XequtJ53Af48k2/L7BJVbeoah7wPpB68m6Aes7z+kDxV9RU4H1VzVXVrcAmZ3smwKT2TODoiXzmbPD/pEhZOflc//pCFm8/zNOje3JZz+Z+32dpRISJw9uz89AJ/pO2y5UMxlQ1n+bgFpEBxS9E5Gx8uxoqAdhZ4vUuZ1lJDwNjRWQXniOW8RVY1wSAAW0b07BOhN/n5z56Ip9rX1vI8p1HeG5MT0b2iPfr/rwZ1CGW5MQYnvt2I7kF1WOqWWPK40uxuBl4XkS2icg2PENFN/mwXmm3zp582cwY4E1VbQ6MAN4RkRAf10VEbhWRNBFJy8gI/uk+g1F4aAgXd2vGzLX7OJbrny4wR47nce1rC1idfpQXftmLi7o188t+KkJEuPf89qQfzWHKop3eVzAmyPlyNdRiVe0BdAd6qGqyqi7xYdu78FxmW6w5/xtmKnYzMNXZzzwgEk/jQl/WRVUnqWqKqqbExsb6EMn4Q2pyPDn5RcxYU/k3qx3OzuOaVxawbk8WL43tzfldmlb6Pk7XgLaN6ZvUkOe+3UROvh1dmOqtvKuhxjp/ThSRiXjafNxc4rU3i4B2ItLKualvNHDydKw7gKHOfjrhKRYZzudGi0gtEWkFtAMWVuxHM1WlV4sGJMTUrvSrog4cy2XMK/PZnHGMV65PYWinJpW6/TMlIkw8vz37s3L59/ztbscxxq/KO7IoPi8RXcajXE5n2juBr4C1eK56Wi0ij4rISOdj9wK3iMhyYDJwg3qsxnPEsQbPlVjjVNW+ugWokBBhZHI83288wMFjuZWyzf1ZOYyZNJ9tB7N5/YY+DGwfmEeO/Vo34py2jXhpzmaO51kzZlN9SaA0gjtTKSkpmpaW5naMGmvd3kwufPp7Hk3twnX9k85oW/sycxjzynz2Hs3htev70L9NYM9/vXj7IS5/cR73XdSR2we2cTuOMRUiIotVNcXb58qc/EhEnilvRVW963SCmeqpY9N6dGwazbRl6WdULPYcPcE1ryxgf2YOb93Ulz5JDSsvpJ/0btmQge1jeXnOZsb2a0ndWuXNKWZMcCpvGGqxl4cxPzMyOZ7F2w+z89Dx01p/1+HjXP3yfA5k5fL2zWcFRaEoNnF4ew4fz+eNuVvdjmKMX5T5FUhV36rKICb4Xdo9nse/XM/05emMG1yxNuE7Dx1n9KT5ZOXk886vziI5McZPKf2jR2IMwzo14ZXvt3Dd2UnUrx3udiRjKpUvM+XFisg/ReQLEfm2+FEV4UxwSWwYRUrLBhWeQW/bgWyuenke2XkFvHdLv6ArFMUmDm9PZk4Br32/xe0oxlQ6X27KexfP1UytgEeAbXguizXmFKnJ8azfl8W6vb5NirQ54xhXT5pHbkER7/2qH10T6vs5of90jq/HiG5Nef2HbRzOznM7jjGVypdi0UhVXwPyVXWOqt4E9PNzLhOkRnRrRmiI8MlS70cXG/dlMXrSfAqLlMm39KNzfD2v6wS6u4e1JzuvgEl2dGGqGV+KRb7z5x4RuVhEeuK5o9qYUzSqW4tz2zXm0+XlT4q0bm8moyfNB+D9W/vRoanXW3eCQvsm0VzaPZ43f9jGgUq658SYQOBLsfiziNTHcwPdb4BXgXv8msoEtVHJCew+coLFOw6X+v7q9KOMmTSf8NAQptzaj7Zx1aNQFJswrB25BYW8NHuz21GMqTS+FIsFqnpUVVep6mBV7a2qJ7ftMOYnwzs3ITI8pNROtCt3HeWaVxZQOzyUKbf1o3VsXRcS+leb2Lpc1rM578zfzr7MHLfjGFMpfCkWP4rI1yJys4g08HsiE/Tq1ApjeOemfL5iz88mB1q28wjXvDqf6MgwptzWn5aNfOl0H5wmDG1HQZHywiybgdhUD750nW2HZ0a7LsBiEfmsuMmgMWVJ7RHP4eP5zN14AIDF2w9z7asLaBAVwZTb+pPYMMrlhP7VolEUV/ZuzuSFO0k/csLtOMacMV+OLFDVhao6Ec9sdYcAu2HPlOu89rHUrx3OtGW7Wbj1ENe9toDG0bWYelt/EmJqux2vStw5pC2K8pwdXZhqwJeb8uqJyPUi8l/gR2APNsWp8SIiLIQR3Zrx5eq9XP/6QprWj2TKrf1oWj/S7WhVpnmDKEb3acHURTtPuwWKMYHClyOL5UAy8KiqtlfV36uq9YYyXo1yJkVKbFib92/tT1y9mlMoio0b3JaQEOGZbza6HcWYM+JLe8zWWl36mJsq1bdVQ16+tjd9kxrSoE6E23Fc0bR+JGPPaslb87bx68FtadW4+p7UN9WbLye4rVCY0yIiXNClaY0tFMXuGNSG8FDhXzM3uB3FmNPm0wluY8zpi42uxfX9k5i2PJ1N+7PcjmPMabFiYUwVuG1gG6LCQ3lqpp27MMHJl6uhHneuiAoXkW9E5IDdZ2FMxTSsE8GN57Ti8xV7WLvHt468xgQSX44szlfVTOASYBfQHvitX1MZUw3dcm5roiPDeGqGnbswwceXYlE85dcIYLKqHvJjHmOqrfpR4fxqQGu+XrOPlbuOuh3HmArxpVh8KiLrgBTgGxGJBaw7mjGn4cYBnilXn7Iro0yQ8eXS2fuA/kCKquYD2UCqv4MZUx3Viwzn1vNa8+26/Swpo4W7MYHIlxPcVwIFqlooIg8A/wbi/Z7MmGrqhrOTaFQnws5dmKDiyzDUg6qaJSIDgAvwNBF80b+xjKm+6tQK4/aBbfh+4wEWbrVTgCY4+FIsCp0/LwZeVNVpQM2+JdeYMzS2X0tio2vxxNfrsSYJJhj4Uix2i8jLwFXAFyJSy8f1jDFlqB0RyrhBbViw9RA/bj7odhxjvPLll/5VwFfAhap6BGiI3WdhzBkb3bcFzepH8uSMDXZ0YQKeL1dDHQc2AxeIyJ1AnKp+7fdkxlRzkeGhjBvclsXbDzNnQ4bbcYwply9XQ00A3gXinMe/RWS8v4MZUxNclZJI8wa17ejCBDxfhqFuBs5S1YdU9SGgH3CLf2MZUzNEhIVw15B2rNh1lJlr97sdx5gy+VIshP9dEYXzXPwTx5ia5xe9EmjZKIonZ2ygqMiOLkxg8qVYvAEsEJGHReRhYD7wml9TGVODhIWGMGFoO9buyeSr1XvdjmNMqXw5wf0kcCNwCDgM3KiqT/uycRG5UETWi8gmEbmvlPefEpFlzmODiBwp8d5jIrLKeVzt+49kTPBJTU6gTWwdnpq5gcJqeHShqtXy56pJyp2DW0RCgBWq2hVYUpENi0go8DwwHE9r80UiMl1V1xR/RlXvKfH58UBP5/nFQC8gGagFzBGR/zqt0o2pdkJDhLuHtWf85KV8tiKd1OQEtyNVivzCIj5ZupvnZ20iMjyU/9zen+jIcO8rmoBT7pGFqhYBy0WkxWlsuy+wSVW3qGoe8D7lNyAcA0x2nncG5qh/2jaNAAAXNElEQVRqgapmA8uBC08jgzFB4+JuzejYNJp/zdxIQWGR23HOSH5hEVMW7WDoE3P47QcriAwPZeP+Y9wzZZmdlwlSvpyzaAasdmbJm1788GG9BGBnide7nGWnEJGWQCvgW2fRcuAiEYkSkcbAYCCxlPVuFZE0EUnLyLDr1E1wC3GOLrYcyOaTZeluxzkteQVFvLdgB4P+MZvff7iSmKhwXr0uhf9OOJeHLunMzLX7eWLGerdjmtNQ7jCU45HT3HZpV0yV9ZViNPCBqhYCqOrXItIH+BHIAOYBBadsTHUSMAkgJSXFvq6YoHdBlyZ0ia/HM99sJDU5nvDQ4Oisk1tQyNS0Xbw4axPpR3PokRjDn0d1ZVCHWEQ8vwqu69+StXsyeX7WZjo2rcelPax5dTAps1iISFugiarOOWn5ecBuH7a9i58fDTQHyvq6NBoYV3KBqv4F+Iuzz/cAm+neVHsiwsTh7bn5rTQ+XLyL0X1PZwS46uTkFzJl0U5enL2ZvZk59GoRw98u78557Rr/VCSKiQiPpnZl0/5j/PaD5bRqXIeuCfVdSm4qqryvLU8DWaUsP+68580ioJ2ItBKRCDwF4ZThKxHpADTAc/RQvCxURBo5z7sD3QFrMWJqhCEd40hOjOHZbzeRW1DofQUX5OQX8sYPWxn4j1n8cfpqEhvW5t83n8WHd5zNwPaxpxSKYhFhIbw4tjcNoyK49e00MrJyqzi5OV3lFYskVV1x8kJVTQOSvG1YVQuAO/E0IVwLTFXV1SLyqIiMLPHRMcD7+vNeB+HA9yKyBs8w01hne8ZUe8VHF7uPnGDqop3eV6hCJ/IKefX7LZz7+Cwe+XQNSY3q8N4tZzH1tv4MKOVoojSx0bWYdF0Kh47ncce/F5NXENwn82sKKasfjYhsUtW2FX3PLSkpKZqWluZ2DGMqhapy1cvz2HHoOHN+O5jI8FBX8xzPK+Df87cz6bstHDiWx9ltGnHX0Hb0a93otLf56fJ0xk9eypi+ifz1sm4+FRpT+URksaqmePtceSe4F4nILar6ykkbvhlYfKYBjTFl8xxddGDMK/N5d8EObh7QypUc2bkFvDN/O698t4WD2XkMaNuYCcPa0Sep4Rlv+9Ie8azb6znh3alZPa7rn3TmgY3flFcs7gY+FpFf8r/ikIJnlrzL/B3MmJquf5tG9G/diBdnb2JM30SiIny5eLFyZOXk8/a87bz6/RYOH8/nvPaxTBjalt4tz7xIlHTv8A6s35vFI5+uoW1sXc5u27hSt28qT5nDUD99QGQw0NV5uVpVvy3v826xYShTHaVtO8QVL83j/os6ctvANn7fX2ZOPm/9sI1X527l6Il8BneI5a6h7ejZooHf9pmVk88vXviRjGO5TB83gBaNovy2L3MqX4ehvBaLYGHFwlRX172+kJW7jvD974dQt5Z/ji6OnsjnjR+28vrcrWTmFDCsUxzjh7SjR2KMX/Z3sm0Hskl9/gea1ovkw1+f7bef05zK12IRHHf8GFODTRzensPH83nzh62Vvu0jx/N48uv1DPj7tzw9cyP9Wjfis/EDePX6PlVWKACSGtfhuWt6snF/FhOtJUhAsvJtTIBLToxhWKc4Jn23hWv7J1G/9pk34jucncdrc7fy5o/bOJZbwIVdmjJ+aFu6xLt3k9y57WL5w8Wd+dNna3j6m41MHN7etSzmVFYsjAkCdw9rzyXPzuW1uVvP6JfowWO5vDp3K2//uI3j+YWM6NqM8UPb0rFpvUpMe/puOieJtXsyeeabjXRqGs1F3Zq5Hck4rFgYEwS6JtTnwi5NeX3uVm46J4mYqIgKrX/gWC6vfLeFd+Zv50R+IZd0j2f8kLa0bxLtp8SnR0T4y2Vd2ZJxjIlTl9OyUR06xwdGIavp7JyFMUHinuHtyc4rYNJ3W3xeZ39WDn/+bA0DHvuWV77fwvmdmzDjnvN4dkzPgCsUxWqFhfLStb2pXzucW95O4+AxawkSCKxYGBMkOjSN5pLu8bz54zavv0D3ZebwyKerOfexWbz+w1ZGdGvGjIkDeXp0T9rGBWaRKCkuOpJJ1/XmwLFc7nh3ibUECQBWLIwJIncPa0dOfiEvzdlc6vt7j+bw8PTVnPv4LN6et52RPeL59t5BPHlVMm1i61Zx2jPTvXkMj1/RnYVbD/HIp6vdjlPj2TkLY4JIm9i6jOqZwNvztnPLua2JqxcJQPqRE7w4ezNTFu2kSJXLezVn3OC2QX+DW2pyAmv2ZPLynC10alaPsf1auh2pxrJiYUyQuWtIO6YtS+eF2Zv51bmteGH2Zv6T5ulOe0XvRH49qA2JDYO7SJT0uws6smFvFg9PX03buLpn1LzQnD67g9uYIPT7D1bw4ZJdiIAgXNWnOXcMaktCTG23o/lFZk4+o57/gSPH85k27pxqVQzdZndwG1ON3TWsHUmN6zCmbwvm/G4Qfx7VrdoWCoB6kZ65vPMLi7jl7TSO59n0NlXNjiyMMUFj9vr93PTmIi7s2pTnr+llc2BUAjuyMMZUO4M6xHH/RZ34YuVenv12k9txahQ7wW2MCSq/OrcVa/dk8uSMDbRvEs2FXZu6HalGsCMLY0xQERH++otu9EiMYeLUZazbm+l2JFcdys5j7R7//x1YsTDGBJ3I8FAmXduburXCuOXtNA5l57kdqcoVFSmTF+5gyBOzuWvyUr+3dbdiYYwJSk3qRfLytb3Zl5nLuHeXkF9Yc1qCrNp9lF+8+CP3f7SS9k2ief6XvQgJ8e/JfisWxpig1bNFA/52WTfmbTnInz9b43Ycvzt6Ip+Hpq1i5HNz2XX4BE9d3YMpt/arkqaQdoLbGBPULu/dnLV7Mnl17lY6NavH6L4t3I5U6VSVj5fu5q9frOVQdh7X9mvJxPM7VMpEWL6yYmGMCXr3XdSR9fuyeHDaKtrE1aVPUkO3I1Wa9Xs9P9fCrYdITozhzRv70jWh6mc0tGEoY0zQCwsN4bkxvWjeIIo7/r2Y3UdOuB3pjB3LLeAvn69hxDPfs2FfFn//RTc+uuNsVwoFWLEwxlQT9aPCeeW63uTmF3Hr22mcyCt0O9JpUVU+X7GHYU/M4ZXvt3Jl7+Z8e+8gRvdt4feT2OWxYmGMqTbaxkXzzJierNmTyW8/WE6wtTPaknGM615fyLj3ltCobgQf/fps/n55dxrWqdg0uv5g5yyMMdXK4I5x/O6Cjjz25To6NavHuMFt3Y7k1Ym8Qp6ftYlJ322hVlgIj4zswth+LQl18UjiZFYsjDHVzu0DW7Nubyb//Ho9HZpEM6xzE7cjlWnmmn08/Olqdh0+wWU9E7h/REfioiPdjnUKKxbGmGpHRHjs8u5sycjm7inL+PjXZ9OuCu5FqIidh47zyKermbl2P+3i6vL+rf0CemInO2dhjKmWIsNDmXRdbyLDQ/nV22kcOR4YLUFyCwp59puNDHtyDj9uPsj/jejIFxPODehCAVYsjDHVWLP6tXn52l7sOZLDne8tpcDlliDfbcjgwqe/54kZGxjaKY5v7h3Iree1ITw08H8VB35CY4w5A71bNuTPo7oyd9MB/vrFOlcy7Dl6gnHvLuG61xeiqrx1U19e+GVvmtUPntkN/XrOQkQuBP4FhAKvqurfT3r/KWCw8zIKiFPVGOe9x4GL8RS0GcAEDbbr4IwxAeGqPoms2ZPJ6z9spVOzaK5MSayS/eYXFvHGD1t5euZGCouUicPbc+t5rYkMD62S/VcmvxULEQkFngeGA7uARSIyXVV/6valqveU+Px4oKfz/GzgHKC78/ZcYCAw2195jTHV2wMXd2LT/mP84eNVtI6tS++WDfy6vwVbDvLgtFVs2HeMoR3jeHhkFxIbRvl1n/7kz2GovsAmVd2iqnnA+0BqOZ8fA0x2nisQCUQAtYBwYJ8fsxpjqrmw0BCeu6YnzWIiuf3fi9lz1D8tQTKycpk4ZRlXT5pPdm4hr1yXwms39AnqQgH+LRYJwM4Sr3c5y04hIi2BVsC3AKo6D5gF7HEeX6nq2lLWu1VE0kQkLSMjo5LjG2Oqm5ioCF65LoXjuQXc+vZicvIrryVIYZHy1o/bGPLEbD5dkc64wW2YOXEgwwP4Ho+K8GexKO3Ww7LOOYwGPlDVQgARaQt0AprjKTBDROS8UzamOklVU1Q1JTY2tpJiG2Oqs/ZNonl6dE9WpR/l9x+uqJSWIEt2HGbkc3P54/TV9Ggew5d3n8dvL+hI7YjgOzdRFn+e4N4FlDyL1BxIL+Ozo4FxJV5fBsxX1WMAIvJfoB/wnR9yGmNqmOGdm3Dv8Pb88+sNdGpWj9sHtjmt7RzOzuPxr9YxeeFOmtSrxXPX9OTibs0QCZw2HZXFn8ViEdBORFoBu/EUhGtO/pCIdAAaAPNKLN4B3CIif8NzhDIQeNqPWY0xNcy4wW1ZuzeLx75cR4cm0QzuGOfzukVFytS0nTz25Toycwr41YBW3D28PXVrVd+mGH77yVS1QETuBL7Cc+ns66q6WkQeBdJUdbrz0THA+yddFvsBMARYiWfo6ktV/dRfWY0xNY+I8I8rurM1I5u7Ji/l43Hn0Daurtf1Vu0+yoPTVrF0xxH6JDXgT6O60rFpvSpI7C6pLrcupKSkaFpamtsxjDFBZveRE4x8di71a4fz8bhzypyqNDMnnye/3sDb87bRsE4E91/UiV/0Sgj6IScRWayqKd4+Z3dwG2NqtISY2rw4tjc7Dx/nrslLKSz6+Rdoz/zXuxjyzzm8NW8bY/u15Jt7B3F57+ZBXygqwoqFMabG69uqIY+mdmXOhgwe+/J/LUE27Mti9KT53DNlOQkNajN93AAeTe1a5tFHdVZ9z8YYY0wFjOnbgrV7Mpn03RZaNIxi56HjvDZ3K3VqhfHXy7oxuk+iq9Oaus2KhTHGOB68pDMb9x3jgU9WAXBVSnN+f2FHGtWt5XIy91mxMMYYR3hoCM//shf/mrmBkcnx9G7Z0O1IAcOKhTHGlNCwTgSPpHZ1O0bAsRPcxhhjvLJiYYwxxisrFsYYY7yyYmGMMcYrKxbGGGO8smJhjDHGKysWxhhjvLJiYYwxxqtq06JcRDKA7WewicbAgUqK42/BlBWCK28wZYXgyhtMWSG48p5J1paq6nVe6mpTLM6UiKT50tM9EARTVgiuvMGUFYIrbzBlheDKWxVZbRjKGGOMV1YsjDHGeGXF4n8muR2gAoIpKwRX3mDKCsGVN5iyQnDl9XtWO2dhjDHGKzuyMMYY41WNLhYikigis0RkrYisFpEJbmcqj4hEishCEVnu5H3E7UzeiEioiCwVkc/czuKNiGwTkZUiskxE0tzOUx4RiRGRD0RknfP/b3+3M5VFRDo4f6fFj0wRudvtXGURkXucf1+rRGSyiES6naksIjLBybna33+nNXoYSkSaAc1UdYmIRAOLgVGqusblaKUSEQHqqOoxEQkH5gITVHW+y9HKJCITgRSgnqpe4nae8ojINiBFVQP+2noReQv4XlVfFZEIIEpVj7idyxsRCQV2A2ep6pncF+UXIpKA599VZ1U9ISJTgS9U9U13k51KRLoC7wN9gTzgS+AOVd3oj/3V6CMLVd2jqkuc51nAWiDB3VRlU49jzstw5xGw1V5EmgMXA6+6naU6EZF6wHnAawCqmhcMhcIxFNgciIWihDCgtoiEAVFAust5ytIJmK+qx1W1AJgDXOavndXoYlGSiCQBPYEF7iYpnzOsswzYD8xQ1UDO+zTwO6DI7SA+UuBrEVksIre6HaYcrYEM4A1niO9VEanjdigfjQYmux2iLKq6G/gnsAPYAxxV1a/dTVWmVcB5ItJIRKKAEUCiv3ZmxQIQkbrAh8Ddqprpdp7yqGqhqiYDzYG+zqFowBGRS4D9qrrY7SwVcI6q9gIuAsaJyHluBypDGNALeFFVewLZwH3uRvLOGS4bCfzH7SxlEZEGQCrQCogH6ojIWHdTlU5V1wKPATPwDEEtBwr8tb8aXyycsf8PgXdV9SO38/jKGXaYDVzocpSynAOMdM4DvA8MEZF/uxupfKqa7vy5H/gYz1hwINoF7CpxVPkBnuIR6C4ClqjqPreDlGMYsFVVM1Q1H/gIONvlTGVS1ddUtZeqngccAvxyvgJqeLFwThi/BqxV1SfdzuONiMSKSIzzvDae/7HXuZuqdKp6v6o2V9UkPEMP36pqQH5DAxCROs5FDjhDOufjOcwPOKq6F9gpIh2cRUOBgLwo4yRjCOAhKMcOoJ+IRDm/H4biOZcZkEQkzvmzBfAL/Pj3G+avDQeJc4BrgZXOeQCA/1PVL1zMVJ5mwFvOFSUhwFRVDfhLUoNEE+Bjz+8HwoD3VPVLdyOVazzwrjO0swW40eU85XLG1IcDt7mdpTyqukBEPgCW4BnSWUpg38n9oYg0AvKBcap62F87qtGXzhpjjPFNjR6GMsYY4xsrFsYYY7yyYmGMMcYrKxbGGGO8smJhjDHGKysWJuCIiIrIEyVe/0ZEHq6kbb8pIldUxra87OdKpxvsrJOWJzk/3/gSy54TkRu8bO92EbnOy2duEJHnynjvWGnLjfGVFQsTiHKBX4hIY7eDlOTc3+Krm4Ffq+rgUt7bD0xw7pHwiaq+pKpvV2D/lcZpqGdqOCsWJhAV4LkR6p6T3zj5yKD4G7OIDBKROSIyVUQ2iMjfReSXzvwfK0WkTYnNDBOR753PXeKsHyoi/xCRRSKyQkRuK7HdWSLyHrCylDxjnO2vEpHHnGUPAQOAl0TkH6X8fBnAN8D1pWyvjYh86TQz/F5EOjrLHxaR3zjP+zgZ5zmZS95pHu+sv1FEHj9p20+IyBIR+UZEYp1lySIy39nex05vJERktoj8VUTm4ClsVzo/43IR+a6Un8lUc1YsTKB6HviliNSvwDo9gAlANzx35rdX1b54WqSPL/G5JGAgnvbpL4lncpub8XQY7QP0AW4RkVbO5/sCf1DVziV3JiLxeBq5DQGSgT4iMkpVHwXSgF+q6m/LyPp34N5SjlYmAeNVtTfwG+CFUtZ9A7hdVfsDhSe9lwxc7fwdXC0ixV1I6+Dpy9QLTyvrPzrL3wZ+r6rd8RTDP5bYVoyqDlTVJ4CHgAtUtQeeZoCmhrFiYQKS0/33beCuCqy2yJmjJBfYDBS3ll6Jp0AUm6qqRc4kMVuAjnh6QV3ntH1ZADQC2jmfX6iqW0vZXx9gttN0rgB4F888E778fFuBhcA1xcuc7sdnA/9xcryMp8ULJT4TA0Sr6o/OovdO2vQ3qnpUVXPw9Itq6SwvAqY4z/8NDHAKcYyqznGWv3VS/iklnv8AvCkitwAVGY4z1YSNRZpA9jSeHj1vlFhWgPMlx2n0VnLcP7fE86ISr4v4+f/rJ/e4UUDwfKP/quQbIjIITwvw0ojXn6B8f8XTMbZ4WCcEOOK0oC+Lt32W/DsopOx/4770+fnp51bV20XkLDxHY8tEJFlVD/qwDVNN2JGFCViqegiYimeIqNg2oLfzPBXPbIEVdaWIhDjnMVoD64GvgDvE07IeEWkv3icUWgAMFJHGznDSGDxDPD5R1XV4vv1f4rzOBLaKyJVOBhGRHietcxjIEpF+zqLRPu4uBCg+13MNMFdVjwKHReRcZ/m1ZeUXkTaqukBVHwIO4MdJdkxgsiMLE+ieAO4s8foVYJqILMRzkrisb/3lWY/nl2ITPGP/OSLyKp6hqiXOEUsGMKq8jajqHhG5H5iF5xv/F6o6rYJZ/oKns2mxXwIvisgDeArh+3gmtSnpZuAVEcnGM6fJUR/2kw10EZHFzuevdpZfj+e8TRTld6/9h4i0w/NzflNKJlPNWddZY4KMiNQtnotdRO4DmqnqBJdjmWrOjiyMCT4XO0c0YcB24AZ345iawI4sjDHGeGUnuI0xxnhlxcIYY4xXViyMMcZ4ZcXCGGOMV1YsjDHGeGXFwhhjjFf/D1BEdMD0o5gCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Cross validation score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6978882833787466\n",
      "Cohen's Kappa = 0.24162836965789736\n",
      "Recall = 0.7314814814814815\n",
      "F1 score = 0.7942154512412034\n",
      "ROC area under curve = 0.6487385036042754\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the top K that was retrieved from the Knn cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors = k_range[((k_scores.index(max(k_scores))))])\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6976263386223185\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for a logistic regression\n",
    "\n",
    "logregression = LogisticRegression(solver='liblinear')\n",
    "CVscores = cross_val_score(logregression, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7797456857402362\n",
      "Cohen's Kappa = 0.37506363645939034\n",
      "Recall = 0.8313390313390313\n",
      "F1 score = 0.8574786952688803\n",
      "ROC area under curve = 0.7042601196963614\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the logregression\n",
    "\n",
    "logregression.fit(X_train, y_train)\n",
    "y_pred = logregression.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train using a Support vector machine, a random forest and a logistic regression, \n",
    "\n",
    "modelSVC.fit(X_train, y_train)\n",
    "modelRF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70943465 0.70952643 0.70622247]\n",
      "[0.791942   0.84104258 0.83452643]\n",
      "0.7099853157121879\n",
      "0.9922601566324033\n"
     ]
    }
   ],
   "source": [
    "#Print cross-validation scores \n",
    "\n",
    "print(cross_val_score(modelSVC, X_train, y_train)) \n",
    "print(cross_val_score(modelRF, X_train, y_train)) \n",
    "\n",
    "#Print model validation scores \n",
    "\n",
    "print(modelSVC.score(X_train, y_train))\n",
    "print(modelRF.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8052906448683016\n",
      "Cohen's Kappa = 0.41210837893022134\n",
      "Recall = 0.8698005698005699\n",
      "F1 score = 0.8768579019171394\n",
      "ROC area under curve = 0.7109069963096809\n",
      "[[ 987  801]\n",
      " [ 914 6106]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy of using the Support Vector Machine\n",
    "y_pred = modelSVC.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7497729336966394\n",
      "Cohen's Kappa = 0.3064761179645996\n",
      "Recall = 0.8037037037037037\n",
      "F1 score = 0.8365954922894425\n",
      "ROC area under curve = 0.6708675118071091\n",
      "[[ 962  826]\n",
      " [1378 5642]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification of accuracy using the Random forest\n",
    "y_pred = modelRF.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(18, input_dim=16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "32688/32688 [==============================] - 1s 30us/step - loss: 0.6351 - acc: 0.6200\n",
      "Epoch 2/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5687 - acc: 0.7007\n",
      "Epoch 3/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5577 - acc: 0.7081\n",
      "Epoch 4/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5530 - acc: 0.7145\n",
      "Epoch 5/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5504 - acc: 0.7164\n",
      "Epoch 6/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5490 - acc: 0.7161\n",
      "Epoch 7/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5477 - acc: 0.7164\n",
      "Epoch 8/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5466 - acc: 0.7173\n",
      "Epoch 9/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5456 - acc: 0.7182\n",
      "Epoch 10/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5450 - acc: 0.7178\n",
      "Epoch 11/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5446 - acc: 0.7184\n",
      "Epoch 12/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5435 - acc: 0.7190\n",
      "Epoch 13/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5435 - acc: 0.7203\n",
      "Epoch 14/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5428 - acc: 0.7185\n",
      "Epoch 15/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5423 - acc: 0.7190\n",
      "Epoch 16/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5421 - acc: 0.7192\n",
      "Epoch 17/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5419 - acc: 0.7174\n",
      "Epoch 18/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5414 - acc: 0.7191\n",
      "Epoch 19/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5410 - acc: 0.7179\n",
      "Epoch 20/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5413 - acc: 0.7189\n",
      "Epoch 21/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5405 - acc: 0.7192\n",
      "Epoch 22/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5402 - acc: 0.7189\n",
      "Epoch 23/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5400 - acc: 0.7197\n",
      "Epoch 24/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5399 - acc: 0.7197\n",
      "Epoch 25/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5395 - acc: 0.7190\n",
      "Epoch 26/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5394 - acc: 0.7191\n",
      "Epoch 27/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5395 - acc: 0.7205\n",
      "Epoch 28/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5389 - acc: 0.7189\n",
      "Epoch 29/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5385 - acc: 0.7201\n",
      "Epoch 30/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5385 - acc: 0.7203\n",
      "Epoch 31/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5382 - acc: 0.7197\n",
      "Epoch 32/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5387 - acc: 0.7193\n",
      "Epoch 33/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5383 - acc: 0.7200\n",
      "Epoch 34/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5380 - acc: 0.7202\n",
      "Epoch 35/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5377 - acc: 0.7206\n",
      "Epoch 36/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5371 - acc: 0.7203\n",
      "Epoch 37/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5374 - acc: 0.7199\n",
      "Epoch 38/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5373 - acc: 0.7208\n",
      "Epoch 39/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5371 - acc: 0.7201\n",
      "Epoch 40/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5366 - acc: 0.7210\n",
      "Epoch 41/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5369 - acc: 0.7212\n",
      "Epoch 42/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5365 - acc: 0.7216\n",
      "Epoch 43/150\n",
      "32688/32688 [==============================] - 0s 10us/step - loss: 0.5361 - acc: 0.7208\n",
      "Epoch 44/150\n",
      "32688/32688 [==============================] - 0s 10us/step - loss: 0.5358 - acc: 0.7228\n",
      "Epoch 45/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5363 - acc: 0.7213\n",
      "Epoch 46/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5358 - acc: 0.7228\n",
      "Epoch 47/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5356 - acc: 0.7223A: 0s - loss: 0.5365 - acc: 0.7\n",
      "Epoch 48/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5357 - acc: 0.7225\n",
      "Epoch 49/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5353 - acc: 0.7216\n",
      "Epoch 50/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5351 - acc: 0.7216\n",
      "Epoch 51/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5351 - acc: 0.7214\n",
      "Epoch 52/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5348 - acc: 0.7223\n",
      "Epoch 53/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5347 - acc: 0.7230\n",
      "Epoch 54/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5352 - acc: 0.7218\n",
      "Epoch 55/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5346 - acc: 0.7209\n",
      "Epoch 56/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5345 - acc: 0.7234\n",
      "Epoch 57/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5338 - acc: 0.7220\n",
      "Epoch 58/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5340 - acc: 0.7224\n",
      "Epoch 59/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5341 - acc: 0.7220\n",
      "Epoch 60/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5341 - acc: 0.7221A: 0s - loss: 0.5351 - acc: 0.72\n",
      "Epoch 61/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5339 - acc: 0.7228\n",
      "Epoch 62/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5342 - acc: 0.7228\n",
      "Epoch 63/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5336 - acc: 0.7224\n",
      "Epoch 64/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5333 - acc: 0.7221\n",
      "Epoch 65/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5335 - acc: 0.7227\n",
      "Epoch 66/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5333 - acc: 0.7230\n",
      "Epoch 67/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5331 - acc: 0.7219\n",
      "Epoch 68/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5334 - acc: 0.7223\n",
      "Epoch 69/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5324 - acc: 0.7242\n",
      "Epoch 70/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5330 - acc: 0.7234\n",
      "Epoch 71/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5326 - acc: 0.7226\n",
      "Epoch 72/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5324 - acc: 0.7239\n",
      "Epoch 73/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5322 - acc: 0.7234\n",
      "Epoch 74/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5325 - acc: 0.7226\n",
      "Epoch 75/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5324 - acc: 0.7226\n",
      "Epoch 76/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5323 - acc: 0.7227\n",
      "Epoch 77/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5320 - acc: 0.7222\n",
      "Epoch 78/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5319 - acc: 0.7232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5323 - acc: 0.7241\n",
      "Epoch 80/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5318 - acc: 0.7233\n",
      "Epoch 81/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5318 - acc: 0.7224\n",
      "Epoch 82/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5317 - acc: 0.7234\n",
      "Epoch 83/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5315 - acc: 0.7230\n",
      "Epoch 84/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5314 - acc: 0.7237\n",
      "Epoch 85/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5315 - acc: 0.7235A: 0s - loss: 0.5316 - acc: 0.722\n",
      "Epoch 86/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5313 - acc: 0.7248\n",
      "Epoch 87/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5311 - acc: 0.7233\n",
      "Epoch 88/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5313 - acc: 0.7247\n",
      "Epoch 89/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5311 - acc: 0.7241\n",
      "Epoch 90/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5309 - acc: 0.7238\n",
      "Epoch 91/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5307 - acc: 0.7248\n",
      "Epoch 92/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5312 - acc: 0.7238\n",
      "Epoch 93/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5307 - acc: 0.7237\n",
      "Epoch 94/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5306 - acc: 0.7253\n",
      "Epoch 95/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5305 - acc: 0.7237\n",
      "Epoch 96/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5305 - acc: 0.7247\n",
      "Epoch 97/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5306 - acc: 0.7244\n",
      "Epoch 98/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5306 - acc: 0.7241\n",
      "Epoch 99/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5299 - acc: 0.7234\n",
      "Epoch 100/150\n",
      "32688/32688 [==============================] - 0s 5us/step - loss: 0.5302 - acc: 0.7235\n",
      "Epoch 101/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5300 - acc: 0.7241\n",
      "Epoch 102/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5294 - acc: 0.7256\n",
      "Epoch 103/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5300 - acc: 0.7242\n",
      "Epoch 104/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5299 - acc: 0.7237A: 0s - loss: 0.5348 - acc: 0.7\n",
      "Epoch 105/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5297 - acc: 0.7246\n",
      "Epoch 106/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5293 - acc: 0.7248\n",
      "Epoch 107/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5290 - acc: 0.7263\n",
      "Epoch 108/150\n",
      "32688/32688 [==============================] - 0s 9us/step - loss: 0.5290 - acc: 0.7245\n",
      "Epoch 109/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5295 - acc: 0.7249\n",
      "Epoch 110/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5289 - acc: 0.7273\n",
      "Epoch 111/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5289 - acc: 0.7266\n",
      "Epoch 112/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5288 - acc: 0.7261\n",
      "Epoch 113/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5286 - acc: 0.7260\n",
      "Epoch 114/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5285 - acc: 0.7267\n",
      "Epoch 115/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5280 - acc: 0.7260\n",
      "Epoch 116/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5281 - acc: 0.7253\n",
      "Epoch 117/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5284 - acc: 0.7265\n",
      "Epoch 118/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5280 - acc: 0.7268\n",
      "Epoch 119/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5283 - acc: 0.7271\n",
      "Epoch 120/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5280 - acc: 0.7258\n",
      "Epoch 121/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5279 - acc: 0.7264\n",
      "Epoch 122/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5279 - acc: 0.7254\n",
      "Epoch 123/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5279 - acc: 0.7264\n",
      "Epoch 124/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5274 - acc: 0.7271\n",
      "Epoch 125/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5272 - acc: 0.7281\n",
      "Epoch 126/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5274 - acc: 0.7265\n",
      "Epoch 127/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5271 - acc: 0.7268\n",
      "Epoch 128/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5273 - acc: 0.7275\n",
      "Epoch 129/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5268 - acc: 0.7278\n",
      "Epoch 130/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5269 - acc: 0.7271\n",
      "Epoch 131/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5266 - acc: 0.7268\n",
      "Epoch 132/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5269 - acc: 0.7276\n",
      "Epoch 133/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5268 - acc: 0.7276\n",
      "Epoch 134/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5268 - acc: 0.7269\n",
      "Epoch 135/150\n",
      "32688/32688 [==============================] - 0s 8us/step - loss: 0.5267 - acc: 0.7276\n",
      "Epoch 136/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5274 - acc: 0.7268\n",
      "Epoch 137/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5266 - acc: 0.7280\n",
      "Epoch 138/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5262 - acc: 0.7279\n",
      "Epoch 139/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5258 - acc: 0.7286\n",
      "Epoch 140/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5259 - acc: 0.7275\n",
      "Epoch 141/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5259 - acc: 0.7281\n",
      "Epoch 142/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5263 - acc: 0.7284\n",
      "Epoch 143/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5256 - acc: 0.7275\n",
      "Epoch 144/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5253 - acc: 0.7291\n",
      "Epoch 145/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5253 - acc: 0.7296\n",
      "Epoch 146/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5254 - acc: 0.7280\n",
      "Epoch 147/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5260 - acc: 0.7286\n",
      "Epoch 148/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5256 - acc: 0.7276A: 0s - loss: 0.5255 - acc: 0.72\n",
      "Epoch 149/150\n",
      "32688/32688 [==============================] - 0s 6us/step - loss: 0.5257 - acc: 0.7283A: 0s - loss: 0.5222 - acc: 0.72\n",
      "Epoch 150/150\n",
      "32688/32688 [==============================] - 0s 7us/step - loss: 0.5255 - acc: 0.7280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b712fe860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the neural network\n",
    "model.fit(X_train, y_train, epochs=150, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7250227066303361\n",
      "Cohen's Kappa = 0.32605715602813223\n",
      "Recall = 0.7383190883190883\n",
      "F1 score = 0.8106036909602753\n",
      "ROC area under curve = 0.7055689401326984\n"
     ]
    }
   ],
   "source": [
    "#make predictions and round answers\n",
    "y_pred = model.predict(X_test)\n",
    "rounded = [round(x[0]) for x in y_pred]\n",
    "y_pred = np.array(rounded, dtype = 'int64')\n",
    "\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best results result from the SVM\n",
    "\n",
    "Accuracy = 0.8052906448683016\n",
    "\n",
    "Cohen's Kappa = 0.41210837893022134\n",
    "\n",
    "Recall = 0.8698005698005699\n",
    "\n",
    "F1 score = 0.8768579019171394\n",
    "\n",
    "ROC area under curve = 0.7109069963096809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
