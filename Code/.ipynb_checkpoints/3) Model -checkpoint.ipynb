{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import whatever will be used in this notebook\n",
    "import pylab\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import SMOTE to balance trainset \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import from sklearn\n",
    "#Estimators:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Set generation \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Performance metrics:\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#K-fold crossvalidation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings regarding the version of the pandas library which is used \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make variable for the imported models\n",
    "modelSVC = SVC()\n",
    "modelRF = RandomForestClassifier()\n",
    "modelLR = LogisticRegression()\n",
    "modelKNN = KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv file and remove the extra ID column\n",
    "credit = pd.read_csv(\"C:\\\\Users\\\\Jeroen\\\\Desktop\\\\Ubiqum\\\\Data Science\\\\Excel Files\\\\credit_3.csv\", header = 0)\n",
    "credit = credit[credit.columns[1:len(credit.columns)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29461</th>\n",
       "      <td>1.849291</td>\n",
       "      <td>0.164656</td>\n",
       "      <td>0.412625</td>\n",
       "      <td>1.132625</td>\n",
       "      <td>0.387036</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29462</th>\n",
       "      <td>-0.682263</td>\n",
       "      <td>-0.234720</td>\n",
       "      <td>-0.127385</td>\n",
       "      <td>-1.004498</td>\n",
       "      <td>0.822032</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29463</th>\n",
       "      <td>-0.656821</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-1.053118</td>\n",
       "      <td>-0.758007</td>\n",
       "      <td>0.169538</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>19.664957</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29464</th>\n",
       "      <td>-0.727253</td>\n",
       "      <td>4.803964</td>\n",
       "      <td>-0.667396</td>\n",
       "      <td>1.388081</td>\n",
       "      <td>0.604534</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>2.813944</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29465</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>-0.220274</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>1.384032</td>\n",
       "      <td>1.148279</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  No_payment  \\\n",
       "29461     1.849291  0.164656  0.412625      1.132625  0.387036    -0.31168   \n",
       "29462    -0.682263 -0.234720 -0.127385     -1.004498  0.822032    -0.31168   \n",
       "29463    -0.656821 -0.344828 -1.053118     -0.758007  0.169538    -0.31168   \n",
       "29464    -0.727253  4.803964 -0.667396      1.388081  0.604534    -0.31168   \n",
       "29465    -0.057084 -0.220274 -0.898829      1.384032  1.148279    -0.31168   \n",
       "\n",
       "       Paid_in_time  Paid_partly  1_month_late  2_months_late  3_months_late  \\\n",
       "29461     -0.488987     0.999729     -0.355373      -0.315466      -0.105112   \n",
       "29462      2.045043    -1.000272     -0.355373      -0.315466      -0.105112   \n",
       "29463     -0.488987    -1.000272     -0.355373      -0.315466      -0.105112   \n",
       "29464     -0.488987    -1.000272      2.813944      -0.315466      -0.105112   \n",
       "29465     -0.488987     0.999729     -0.355373      -0.315466      -0.105112   \n",
       "\n",
       "       4_months_late  5_months_late  6_months_late  7_months_late  \\\n",
       "29461      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "29462      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "29463      19.664957      -0.029718      -0.019325      -0.017479   \n",
       "29464      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "29465      -0.050852      -0.029718      -0.019325      -0.017479   \n",
       "\n",
       "       8_months_late  DEFAULT  \n",
       "29461      -0.025401        1  \n",
       "29462      -0.025401        1  \n",
       "29463      -0.025401        0  \n",
       "29464      -0.025401        0  \n",
       "29465      -0.025401        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Give head of the df so it is easily visible which vars should function as independent variable\n",
    "credit.head()\n",
    "credit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.652117</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-1.130262</td>\n",
       "      <td>-0.651099</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>3.169916</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.668758</td>\n",
       "      <td>-0.344828</td>\n",
       "      <td>-0.358818</td>\n",
       "      <td>-0.999086</td>\n",
       "      <td>-1.026701</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.309746</td>\n",
       "      <td>-0.253840</td>\n",
       "      <td>-0.590251</td>\n",
       "      <td>-0.649230</td>\n",
       "      <td>-0.156709</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.069778</td>\n",
       "      <td>-0.224950</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>1.350797</td>\n",
       "      <td>0.169538</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>-0.488987</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.588526</td>\n",
       "      <td>-0.224950</td>\n",
       "      <td>-0.898829</td>\n",
       "      <td>-0.754215</td>\n",
       "      <td>2.344518</td>\n",
       "      <td>-0.31168</td>\n",
       "      <td>2.045043</td>\n",
       "      <td>-1.000272</td>\n",
       "      <td>-0.355373</td>\n",
       "      <td>-0.315466</td>\n",
       "      <td>-0.105112</td>\n",
       "      <td>-0.050852</td>\n",
       "      <td>-0.029718</td>\n",
       "      <td>-0.019325</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>-0.025401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUR_BIL_SEP  PAID_SEP    CREDIT  CREDIT_RATIO       AGE  No_payment  \\\n",
       "0    -0.652117 -0.344828 -1.130262     -0.651099 -1.244199    -0.31168   \n",
       "1    -0.668758 -0.344828 -0.358818     -0.999086 -1.026701    -0.31168   \n",
       "2    -0.309746 -0.253840 -0.590251     -0.649230 -0.156709    -0.31168   \n",
       "3    -0.069778 -0.224950 -0.898829      1.350797  0.169538    -0.31168   \n",
       "4    -0.588526 -0.224950 -0.898829     -0.754215  2.344518    -0.31168   \n",
       "\n",
       "   Paid_in_time  Paid_partly  1_month_late  2_months_late  3_months_late  \\\n",
       "0     -0.488987    -1.000272     -0.355373       3.169916      -0.105112   \n",
       "1      2.045043    -1.000272     -0.355373      -0.315466      -0.105112   \n",
       "2     -0.488987     0.999729     -0.355373      -0.315466      -0.105112   \n",
       "3     -0.488987     0.999729     -0.355373      -0.315466      -0.105112   \n",
       "4      2.045043    -1.000272     -0.355373      -0.315466      -0.105112   \n",
       "\n",
       "   4_months_late  5_months_late  6_months_late  7_months_late  8_months_late  \n",
       "0      -0.050852      -0.029718      -0.019325      -0.017479      -0.025401  \n",
       "1      -0.050852      -0.029718      -0.019325      -0.017479      -0.025401  \n",
       "2      -0.050852      -0.029718      -0.019325      -0.017479      -0.025401  \n",
       "3      -0.050852      -0.029718      -0.019325      -0.017479      -0.025401  \n",
       "4      -0.050852      -0.029718      -0.019325      -0.017479      -0.025401  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the features that will function as independent variables:\n",
    "credit_indep = credit.iloc[:, 0:(len(credit.columns)-1)]\n",
    "credit_indep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: DEFAULT, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the dependent variable and turn it into a seperate vector:\n",
    "credit_dep = credit['DEFAULT']\n",
    "credit_dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUR_BIL_SEP</th>\n",
       "      <th>PAID_SEP</th>\n",
       "      <th>CREDIT</th>\n",
       "      <th>CREDIT_RATIO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>No_payment</th>\n",
       "      <th>Paid_in_time</th>\n",
       "      <th>Paid_partly</th>\n",
       "      <th>1_month_late</th>\n",
       "      <th>2_months_late</th>\n",
       "      <th>3_months_late</th>\n",
       "      <th>4_months_late</th>\n",
       "      <th>5_months_late</th>\n",
       "      <th>6_months_late</th>\n",
       "      <th>7_months_late</th>\n",
       "      <th>8_months_late</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>2.946600e+04</td>\n",
       "      <td>29466.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.103345e-16</td>\n",
       "      <td>2.656975e-16</td>\n",
       "      <td>-3.281235e-15</td>\n",
       "      <td>3.921537e-16</td>\n",
       "      <td>4.803280e-16</td>\n",
       "      <td>1.062419e-14</td>\n",
       "      <td>2.150282e-15</td>\n",
       "      <td>1.553156e-15</td>\n",
       "      <td>1.034401e-15</td>\n",
       "      <td>-2.365040e-15</td>\n",
       "      <td>5.959323e-15</td>\n",
       "      <td>-7.981141e-16</td>\n",
       "      <td>-7.367930e-16</td>\n",
       "      <td>-4.664973e-16</td>\n",
       "      <td>-8.970036e-16</td>\n",
       "      <td>-8.155870e-15</td>\n",
       "      <td>0.792914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>0.405225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.943419e+00</td>\n",
       "      <td>-3.448283e-01</td>\n",
       "      <td>-1.207406e+00</td>\n",
       "      <td>-4.485552e+00</td>\n",
       "      <td>-1.570446e+00</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>-1.000272e+00</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.490855e-01</td>\n",
       "      <td>-2.847691e-01</td>\n",
       "      <td>-8.988289e-01</td>\n",
       "      <td>-9.815463e-01</td>\n",
       "      <td>-8.092031e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>-1.000272e+00</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.848821e-01</td>\n",
       "      <td>-2.132614e-01</td>\n",
       "      <td>-2.045296e-01</td>\n",
       "      <td>-2.549452e-01</td>\n",
       "      <td>-1.567091e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.204590e-01</td>\n",
       "      <td>-4.252409e-02</td>\n",
       "      <td>5.669140e-01</td>\n",
       "      <td>9.783657e-01</td>\n",
       "      <td>6.045339e-01</td>\n",
       "      <td>-3.116795e-01</td>\n",
       "      <td>-4.889872e-01</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>-3.553731e-01</td>\n",
       "      <td>-3.154657e-01</td>\n",
       "      <td>-1.051123e-01</td>\n",
       "      <td>-5.085188e-02</td>\n",
       "      <td>-2.971788e-02</td>\n",
       "      <td>-1.932488e-02</td>\n",
       "      <td>-1.747942e-02</td>\n",
       "      <td>-2.540132e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.233379e+01</td>\n",
       "      <td>5.201533e+01</td>\n",
       "      <td>6.429886e+00</td>\n",
       "      <td>1.474319e+01</td>\n",
       "      <td>4.736996e+00</td>\n",
       "      <td>3.208424e+00</td>\n",
       "      <td>2.045043e+00</td>\n",
       "      <td>9.997285e-01</td>\n",
       "      <td>2.813944e+00</td>\n",
       "      <td>3.169916e+00</td>\n",
       "      <td>9.513638e+00</td>\n",
       "      <td>1.966496e+01</td>\n",
       "      <td>3.364978e+01</td>\n",
       "      <td>5.174676e+01</td>\n",
       "      <td>5.721014e+01</td>\n",
       "      <td>3.936803e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CUR_BIL_SEP      PAID_SEP        CREDIT  CREDIT_RATIO           AGE  \\\n",
       "count  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04   \n",
       "mean  -2.103345e-16  2.656975e-16 -3.281235e-15  3.921537e-16  4.803280e-16   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -2.943419e+00 -3.448283e-01 -1.207406e+00 -4.485552e+00 -1.570446e+00   \n",
       "25%   -6.490855e-01 -2.847691e-01 -8.988289e-01 -9.815463e-01 -8.092031e-01   \n",
       "50%   -3.848821e-01 -2.132614e-01 -2.045296e-01 -2.549452e-01 -1.567091e-01   \n",
       "75%    2.204590e-01 -4.252409e-02  5.669140e-01  9.783657e-01  6.045339e-01   \n",
       "max    1.233379e+01  5.201533e+01  6.429886e+00  1.474319e+01  4.736996e+00   \n",
       "\n",
       "         No_payment  Paid_in_time   Paid_partly  1_month_late  2_months_late  \\\n",
       "count  2.946600e+04  2.946600e+04  2.946600e+04  2.946600e+04   2.946600e+04   \n",
       "mean   1.062419e-14  2.150282e-15  1.553156e-15  1.034401e-15  -2.365040e-15   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   1.000017e+00   \n",
       "min   -3.116795e-01 -4.889872e-01 -1.000272e+00 -3.553731e-01  -3.154657e-01   \n",
       "25%   -3.116795e-01 -4.889872e-01 -1.000272e+00 -3.553731e-01  -3.154657e-01   \n",
       "50%   -3.116795e-01 -4.889872e-01  9.997285e-01 -3.553731e-01  -3.154657e-01   \n",
       "75%   -3.116795e-01 -4.889872e-01  9.997285e-01 -3.553731e-01  -3.154657e-01   \n",
       "max    3.208424e+00  2.045043e+00  9.997285e-01  2.813944e+00   3.169916e+00   \n",
       "\n",
       "       3_months_late  4_months_late  5_months_late  6_months_late  \\\n",
       "count   2.946600e+04   2.946600e+04   2.946600e+04   2.946600e+04   \n",
       "mean    5.959323e-15  -7.981141e-16  -7.367930e-16  -4.664973e-16   \n",
       "std     1.000017e+00   1.000017e+00   1.000017e+00   1.000017e+00   \n",
       "min    -1.051123e-01  -5.085188e-02  -2.971788e-02  -1.932488e-02   \n",
       "25%    -1.051123e-01  -5.085188e-02  -2.971788e-02  -1.932488e-02   \n",
       "50%    -1.051123e-01  -5.085188e-02  -2.971788e-02  -1.932488e-02   \n",
       "75%    -1.051123e-01  -5.085188e-02  -2.971788e-02  -1.932488e-02   \n",
       "max     9.513638e+00   1.966496e+01   3.364978e+01   5.174676e+01   \n",
       "\n",
       "       7_months_late  8_months_late       DEFAULT  \n",
       "count   2.946600e+04   2.946600e+04  29466.000000  \n",
       "mean   -8.970036e-16  -8.155870e-15      0.792914  \n",
       "std     1.000017e+00   1.000017e+00      0.405225  \n",
       "min    -1.747942e-02  -2.540132e-02      0.000000  \n",
       "25%    -1.747942e-02  -2.540132e-02      1.000000  \n",
       "50%    -1.747942e-02  -2.540132e-02      1.000000  \n",
       "75%    -1.747942e-02  -2.540132e-02      1.000000  \n",
       "max     5.721014e+01   3.936803e+01      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20626, 16) (20626,)\n",
      "(8840, 16) (8840,)\n",
      "[ 4285 16341]\n"
     ]
    }
   ],
   "source": [
    "#generate a dataframe that functions as a trainingset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit_indep, credit_dep, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance the data\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32682, 16) (32682,)\n",
      "(8840, 16) (8840,)\n",
      "[16341 16341]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011761535265517\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for k = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.802001036835742, 0.818647228803821, 0.7900686482581534, 0.8011761535265517, 0.7798489850613304, 0.7887840200030694, 0.7756574174929536, 0.7793904940503595]\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data to select optimal K \n",
    "k_range = range(2,10)\n",
    "k_scores= []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    CVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "    k_scores.append(CVscores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross validation score')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Cross validation score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6941176470588235\n",
      "Cohen's Kappa = 0.2340270790726071\n",
      "Recall = 0.7297451231667379\n",
      "F1 score = 0.7912613864443414\n",
      "ROC area under curve = 0.6430783953753337\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the top K that was retrieved from the Knn cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors = k_range[((k_scores.index(max(k_scores))))])\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697234212585015\n"
     ]
    }
   ],
   "source": [
    "# Do a Kfold cross validation on the training data for a logistic regression\n",
    "\n",
    "logregression = LogisticRegression(solver='liblinear')\n",
    "CVscores = cross_val_score(logregression, X_train, y_train, cv = 10, scoring = \"accuracy\")\n",
    "print(CVscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7856334841628959\n",
      "Cohen's Kappa = 0.3672424708056272\n",
      "Recall = 0.8520575252740994\n",
      "F1 score = 0.8633052008944672\n",
      "ROC area under curve = 0.6904756531158609\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy using the logregression\n",
    "\n",
    "logregression.fit(X_train, y_train)\n",
    "y_pred = logregression.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train using a Support vector machine, a random forest and a logistic regression, \n",
    "\n",
    "modelSVC.fit(X_train, y_train)\n",
    "modelRF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70727006 0.70901414 0.70442445]\n",
      "[0.78226547 0.83504681 0.82816229]\n",
      "0.7087387552781348\n",
      "0.9912490055688147\n"
     ]
    }
   ],
   "source": [
    "#Print cross-validation scores \n",
    "\n",
    "print(cross_val_score(modelSVC, X_train, y_train)) \n",
    "print(cross_val_score(modelRF, X_train, y_train)) \n",
    "\n",
    "#Print model validation scores \n",
    "\n",
    "print(modelSVC.score(X_train, y_train))\n",
    "print(modelRF.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8083710407239819\n",
      "Cohen's Kappa = 0.40719714858493716\n",
      "Recall = 0.8829560017086715\n",
      "F1 score = 0.8798240635641317\n",
      "ROC area under curve = 0.7015220294729378\n",
      "[[ 945  872]\n",
      " [ 822 6201]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy of using the Support Vector Machine\n",
    "y_pred = modelSVC.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7452488687782806\n",
      "Cohen's Kappa = 0.30451959830854547\n",
      "Recall = 0.7972376477288908\n",
      "F1 score = 0.8325650557620818\n",
      "ROC area under curve = 0.6707707225986227\n",
      "[[ 989  828]\n",
      " [1424 5599]]\n"
     ]
    }
   ],
   "source": [
    "# Check classification of accuracy using the Random forest\n",
    "y_pred = modelRF.predict(X_test)\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(18, input_dim=16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Jeroen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "32682/32682 [==============================] - 1s 32us/step - loss: 0.6110 - acc: 0.6462\n",
      "Epoch 2/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5663 - acc: 0.7022\n",
      "Epoch 3/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5608 - acc: 0.7031\n",
      "Epoch 4/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5580 - acc: 0.7081\n",
      "Epoch 5/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5563 - acc: 0.7095\n",
      "Epoch 6/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5554 - acc: 0.7105\n",
      "Epoch 7/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5538 - acc: 0.7117\n",
      "Epoch 8/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5532 - acc: 0.7111\n",
      "Epoch 9/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5520 - acc: 0.7114\n",
      "Epoch 10/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5514 - acc: 0.7121\n",
      "Epoch 11/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5506 - acc: 0.7125\n",
      "Epoch 12/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5504 - acc: 0.7140\n",
      "Epoch 13/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5498 - acc: 0.7145\n",
      "Epoch 14/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5496 - acc: 0.7122\n",
      "Epoch 15/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5491 - acc: 0.7122\n",
      "Epoch 16/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5489 - acc: 0.7123\n",
      "Epoch 17/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5484 - acc: 0.7125\n",
      "Epoch 18/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5481 - acc: 0.7135\n",
      "Epoch 19/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5477 - acc: 0.7138\n",
      "Epoch 20/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5474 - acc: 0.7146\n",
      "Epoch 21/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5474 - acc: 0.7132\n",
      "Epoch 22/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5470 - acc: 0.7153\n",
      "Epoch 23/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5467 - acc: 0.7146\n",
      "Epoch 24/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5465 - acc: 0.7150\n",
      "Epoch 25/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5465 - acc: 0.7161\n",
      "Epoch 26/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5461 - acc: 0.7146\n",
      "Epoch 27/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5458 - acc: 0.7158\n",
      "Epoch 28/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5455 - acc: 0.7142\n",
      "Epoch 29/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5453 - acc: 0.7157\n",
      "Epoch 30/150\n",
      "32682/32682 [==============================] - 0s 10us/step - loss: 0.5453 - acc: 0.7153\n",
      "Epoch 31/150\n",
      "32682/32682 [==============================] - 0s 11us/step - loss: 0.5450 - acc: 0.7159\n",
      "Epoch 32/150\n",
      "32682/32682 [==============================] - 0s 11us/step - loss: 0.5445 - acc: 0.7172\n",
      "Epoch 33/150\n",
      "32682/32682 [==============================] - 0s 10us/step - loss: 0.5442 - acc: 0.7166\n",
      "Epoch 34/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5441 - acc: 0.7162\n",
      "Epoch 35/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5442 - acc: 0.7174\n",
      "Epoch 36/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5434 - acc: 0.7166\n",
      "Epoch 37/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5432 - acc: 0.7188\n",
      "Epoch 38/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5432 - acc: 0.7172\n",
      "Epoch 39/150\n",
      "32682/32682 [==============================] - 0s 10us/step - loss: 0.5432 - acc: 0.7187\n",
      "Epoch 40/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5435 - acc: 0.7172\n",
      "Epoch 41/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5429 - acc: 0.7178\n",
      "Epoch 42/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5426 - acc: 0.7186\n",
      "Epoch 43/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5424 - acc: 0.7186\n",
      "Epoch 44/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5425 - acc: 0.7176\n",
      "Epoch 45/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5420 - acc: 0.7190\n",
      "Epoch 46/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5425 - acc: 0.7181\n",
      "Epoch 47/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5418 - acc: 0.7199\n",
      "Epoch 48/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5416 - acc: 0.7194\n",
      "Epoch 49/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5413 - acc: 0.7205\n",
      "Epoch 50/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5411 - acc: 0.7202\n",
      "Epoch 51/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5410 - acc: 0.7194\n",
      "Epoch 52/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5407 - acc: 0.7196\n",
      "Epoch 53/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5406 - acc: 0.7201\n",
      "Epoch 54/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5406 - acc: 0.7201\n",
      "Epoch 55/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5404 - acc: 0.7198\n",
      "Epoch 56/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5403 - acc: 0.7216\n",
      "Epoch 57/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5399 - acc: 0.7203\n",
      "Epoch 58/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5395 - acc: 0.7199\n",
      "Epoch 59/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5397 - acc: 0.7192\n",
      "Epoch 60/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5395 - acc: 0.7194\n",
      "Epoch 61/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5391 - acc: 0.7200\n",
      "Epoch 62/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5391 - acc: 0.7207\n",
      "Epoch 63/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5392 - acc: 0.7202\n",
      "Epoch 64/150\n",
      "32682/32682 [==============================] - 0s 10us/step - loss: 0.5388 - acc: 0.7198\n",
      "Epoch 65/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5387 - acc: 0.7191\n",
      "Epoch 66/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5390 - acc: 0.7197\n",
      "Epoch 67/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5389 - acc: 0.7209\n",
      "Epoch 68/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5384 - acc: 0.7192\n",
      "Epoch 69/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5384 - acc: 0.7206\n",
      "Epoch 70/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5379 - acc: 0.7207\n",
      "Epoch 71/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5380 - acc: 0.7210\n",
      "Epoch 72/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5378 - acc: 0.7205\n",
      "Epoch 73/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5379 - acc: 0.7210\n",
      "Epoch 74/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5377 - acc: 0.7217\n",
      "Epoch 75/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5380 - acc: 0.7201\n",
      "Epoch 76/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5371 - acc: 0.7214\n",
      "Epoch 77/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5370 - acc: 0.7217\n",
      "Epoch 78/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5370 - acc: 0.7213\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5368 - acc: 0.7221\n",
      "Epoch 80/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5371 - acc: 0.7207\n",
      "Epoch 81/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5366 - acc: 0.7211\n",
      "Epoch 82/150\n",
      "32682/32682 [==============================] - 0s 9us/step - loss: 0.5365 - acc: 0.7225\n",
      "Epoch 83/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5366 - acc: 0.7212\n",
      "Epoch 84/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5364 - acc: 0.7219\n",
      "Epoch 85/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5362 - acc: 0.7224\n",
      "Epoch 86/150\n",
      "32682/32682 [==============================] - 0s 10us/step - loss: 0.5361 - acc: 0.7215\n",
      "Epoch 87/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5359 - acc: 0.7222\n",
      "Epoch 88/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5355 - acc: 0.7220\n",
      "Epoch 89/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5357 - acc: 0.7214\n",
      "Epoch 90/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5357 - acc: 0.7218\n",
      "Epoch 91/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5356 - acc: 0.7215\n",
      "Epoch 92/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5353 - acc: 0.7213\n",
      "Epoch 93/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5354 - acc: 0.7219\n",
      "Epoch 94/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5352 - acc: 0.7229\n",
      "Epoch 95/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5347 - acc: 0.7236\n",
      "Epoch 96/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5349 - acc: 0.7213\n",
      "Epoch 97/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5349 - acc: 0.7223\n",
      "Epoch 98/150\n",
      "32682/32682 [==============================] - 0s 8us/step - loss: 0.5342 - acc: 0.7216\n",
      "Epoch 99/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5343 - acc: 0.7225\n",
      "Epoch 100/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5341 - acc: 0.7223\n",
      "Epoch 101/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5341 - acc: 0.7232\n",
      "Epoch 102/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5336 - acc: 0.7224\n",
      "Epoch 103/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5335 - acc: 0.7227\n",
      "Epoch 104/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5334 - acc: 0.7228\n",
      "Epoch 105/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5337 - acc: 0.7226\n",
      "Epoch 106/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5332 - acc: 0.7222\n",
      "Epoch 107/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5331 - acc: 0.7223\n",
      "Epoch 108/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5336 - acc: 0.7219\n",
      "Epoch 109/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5330 - acc: 0.7215\n",
      "Epoch 110/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5330 - acc: 0.7235\n",
      "Epoch 111/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5329 - acc: 0.7240\n",
      "Epoch 112/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5327 - acc: 0.7239\n",
      "Epoch 113/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5326 - acc: 0.7236\n",
      "Epoch 114/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5328 - acc: 0.7237\n",
      "Epoch 115/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5327 - acc: 0.7237\n",
      "Epoch 116/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5324 - acc: 0.7235\n",
      "Epoch 117/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5321 - acc: 0.7246\n",
      "Epoch 118/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5323 - acc: 0.7238\n",
      "Epoch 119/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5324 - acc: 0.7230\n",
      "Epoch 120/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5316 - acc: 0.7251\n",
      "Epoch 121/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5318 - acc: 0.7235\n",
      "Epoch 122/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5319 - acc: 0.7240\n",
      "Epoch 123/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5324 - acc: 0.7235\n",
      "Epoch 124/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5317 - acc: 0.7246\n",
      "Epoch 125/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5317 - acc: 0.7255\n",
      "Epoch 126/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5315 - acc: 0.7257\n",
      "Epoch 127/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5313 - acc: 0.7236\n",
      "Epoch 128/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5316 - acc: 0.7258\n",
      "Epoch 129/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5317 - acc: 0.7251\n",
      "Epoch 130/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5314 - acc: 0.7243\n",
      "Epoch 131/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5314 - acc: 0.7263\n",
      "Epoch 132/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5311 - acc: 0.7235\n",
      "Epoch 133/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5307 - acc: 0.7250\n",
      "Epoch 134/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5314 - acc: 0.7260\n",
      "Epoch 135/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5313 - acc: 0.7269\n",
      "Epoch 136/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5310 - acc: 0.7262\n",
      "Epoch 137/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5313 - acc: 0.7239\n",
      "Epoch 138/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5310 - acc: 0.7247\n",
      "Epoch 139/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5308 - acc: 0.7259\n",
      "Epoch 140/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5309 - acc: 0.7246\n",
      "Epoch 141/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5305 - acc: 0.7254\n",
      "Epoch 142/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5306 - acc: 0.7257\n",
      "Epoch 143/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5310 - acc: 0.7259\n",
      "Epoch 144/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5305 - acc: 0.7254\n",
      "Epoch 145/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5306 - acc: 0.7246\n",
      "Epoch 146/150\n",
      "32682/32682 [==============================] - 0s 7us/step - loss: 0.5303 - acc: 0.7267\n",
      "Epoch 147/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5303 - acc: 0.7263\n",
      "Epoch 148/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5307 - acc: 0.7259\n",
      "Epoch 149/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5298 - acc: 0.7269\n",
      "Epoch 150/150\n",
      "32682/32682 [==============================] - 0s 6us/step - loss: 0.5300 - acc: 0.7267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f99b41cb00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the neural network\n",
    "model.fit(X_train, y_train, epochs=150, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7684389140271494\n",
      "Cohen's Kappa = 0.3672539177814871\n",
      "Recall = 0.8121885234230386\n",
      "F1 score = 0.8478632478632478\n",
      "ROC area under curve = 0.7057640470720036\n"
     ]
    }
   ],
   "source": [
    "#make predictions and round answers\n",
    "y_pred = model.predict(X_test)\n",
    "rounded = [round(x[0]) for x in y_pred]\n",
    "y_pred = np.array(rounded, dtype = 'int64')\n",
    "\n",
    "print(\"Accuracy =\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Cohen's Kappa =\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Recall =\", recall_score(y_test, y_pred))\n",
    "print(\"F1 score =\", f1_score(y_test, y_pred))\n",
    "print(\"ROC area under curve =\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best results result from the SVM\n",
    "\n",
    "Accuracy = 0.8079185520361991\n",
    "\n",
    "Cohen's Kappa = 0.4065312121157727\n",
    "\n",
    "Recall = 0.8822440552470454\n",
    "\n",
    "F1 score = 0.8794889992902768\n",
    "\n",
    "ROC area under curve = 0.7014412351083879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
